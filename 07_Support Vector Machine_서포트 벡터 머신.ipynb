{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation)\n",
    "Deep Learning이 활개하는 지금, 고전 알고리즘들이 맥을 못추는 와중에도, 그래도 수학적으로도 훌륭한 알고리즘이 있는데, 그것이 바로 SVM(서포트 벡터 머신)이다.\n",
    "\n",
    "다음의 선형분리가능한(Linearly Seperable) 데이터를 분류하는 경우를 보자.\n",
    "\n",
    "- Figure.1 세가지의 분류 중 어느 분류가 가장 잘 되었다고 볼 수 있는가?\n",
    "![](./image/07/07_01.jpg)\n",
    "이미지 출저:Machine Learning:An Algorithm Perspective 2nd edition_Stephen Marsland\n",
    "\n",
    "아마, 모두 두번째의 경우가 주어진 데이터를 가장 잘 분류했다고 할 것이다. 어떤 기준에서 두번째를 선택하였는지, 왜 두번째가 좋은지를 생각해보자.\n",
    "\n",
    "그 기준을 분류선과 데이터 사이의 여유의 정도(?)를 측정할 수 있다면, 가장 여유롭게 분류하는 게 좋다고 이야기 할 수 있다.\n",
    "\n",
    "그러한 여유의 정도를 마진(Margin)이라고 한다.\n",
    "\n",
    "## Definition) 마진(Margin, Denoted by $M$)\n",
    "\n",
    "The largest radius around the line for which we have a no-man's land where no datapoints lie within the region.\n",
    "\n",
    "분류선을 기준으로, 데이터가 포함되지 않는 영역의 반경을 마진이라고 한다. \n",
    "\n",
    "- Figure.2 Margin, 마진\n",
    "![](./image/07/07_02.png)\n",
    "이미지 출저:Machine Learning:An Algorithm Perspective 2nd edition_Stephen Marsland\n",
    "\n",
    "그래서, 우리는 분류선(3차원 이상에서는 Hyperplane,초평면)을 주어진 데이터에 대해서 마진을 가장 크게 하는 것으로 찾고자 한다.\n",
    "\n",
    "그렇다면 마진을 어떻게 수치화하여 나타낼 것인가를 생각해보자.\n",
    "\n",
    "## Definition) Support Vector\n",
    "\n",
    "The datapoints in each class that lie closest th the classification line\n",
    "\n",
    "주어진 분류선을 기준으로 분류선과 가장 가까운 각 분류에 속한 데이터 점들을 서포트 벡터라고 한다.\n",
    "\n",
    "- Figure.3 Support Vector, 서포트 벡터\n",
    "![](./image/07/07_03.png)\n",
    "이미지 출저:http://www.saedsayad.com/support_vector_machine.htm\n",
    "\n",
    "그렇다면 서포트 벡터 머신이란, 서포트 벡터의 중심을 관통하는 분류선을 찾는것이며, \n",
    "\n",
    "이는 마진이 최대가 되게하는 분류선의 기울기를 찾음으로써 구할 수 있다.\n",
    "\n",
    "우선 가장 간단한 두개의 클래스를 가진 N개의 데이터를 분류하는 문제를 생각해보자.\n",
    "\n",
    "2개의 클래스 ('$+$', 'o')에 대해 '$+$'클래스에 있는 데이터에 대해서는 양의 값을,\n",
    "\n",
    "'o' 클래스에 있는 데이터에 대해서는 음의 값을 갖게 하는 분류선 $ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 $을 찾고자 한다.\n",
    "\n",
    "이 때, 주어진 학습데이터의 클래스 중 분류선과 가장 가까이 있는 학습 데이터 즉, 서포트 벡터에 대해서 다음의 식이 성립한다고 하자.\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x_+} + b = 1\n",
    "\\\\ \\mathbf{w} \\cdot \\mathbf{x_o} + b = -1$$\n",
    "- Figure 4.\n",
    "![](./image/07/07_04.png)\n",
    "이미지 출저:J.H. Lee, Lecture note:http://bigs.skku.edu/bigs/menu4/reference.jsp?mode=view&article_no=303114&board_wrapper=%2Fbigs%2Fmenu4%2Freference.jsp&pager.offset=10&board_no=1423\n",
    "\n",
    "이 때, 주어진 분류선의 마진 M은 $(\\mathbf{x_+}-\\mathbf{x_o})$와 분류선의 법선 유닛 벡터($\\frac{\\mathbf{w}}{||\\mathbf{w}||}$)을 이용하여 계산 할 수 있다.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    " 2M \n",
    " & = (\\mathbf{x_+}-\\mathbf{x_o}) \\cdot \\frac{\\mathbf{w}}{||\\mathbf{w}||}\\\\ \n",
    " & = \\frac{1}{||\\mathbf{w}||}(\\mathbf{w} \\cdot \\mathbf{x_+} - \\mathbf{w} \\cdot \\mathbf{x_o} )\\\\\n",
    " & = \\frac{1}{||\\mathbf{w}||}((1-b)-(-1-b))\\\\\n",
    " & = \\frac{2}{||\\mathbf{w}||}\\\\\n",
    "\\therefore \\quad M & = \\frac{1}{||\\mathbf{w}||}\n",
    "\\end{aligned}\n",
    "\n",
    "- Figure 5.\n",
    "![](./image/07/07_05.png)\n",
    "\n",
    "그러므로 우리는 주어진 분류선에 대해 마진 값 M을 계산할 수 있다.\n",
    "\n",
    "우리는 이 마진 값을 최대로 하는 기울기($\\mathbf{w}$)와 절편($b$)을 찾는 문제가 된다.\n",
    "\n",
    "그리고 이 분류선이 기존의 학습데이터에 대해서는 잘 분류를 해야하므로,'$+$' 클래스의 타겟 값($y$)을 $+1$, 'o' 클래스의 타겟 값을 $-1$이라고 한다면, \n",
    "\n",
    "이 분류선이 데이터를 잘 분류한다는 조건을 다음과 같이 쓸 수 있다.\n",
    "\n",
    "$$ y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}$$\n",
    "\n",
    "이를 정리하면, SVM은 부등식 제약조건이 있는 목적함수의 최대값을 찾는 최적화 문제로 표현할 수 있다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad \\frac{1}{||\\mathbf{w}||}\n",
    "\\\\ & subject \\: to \\quad y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "여기서 $\\frac{1}{||\\mathbf{w}||}$을 최대로 하는 것은 $||\\mathbf{w}||$의 값을 최소로 하는 것과 동치이므로, 최소값을 찾는 문제로 적으면,\n",
    "\n",
    "\\begin{aligned}\n",
    "&min \\quad \\frac{1}{2} ||\\mathbf{w}||^2\n",
    "\\\\ & subject \\: to \\quad y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "이제 우리는 convex optimization 문제를 얻어냈다.\n",
    "\n",
    "이 문제의 Lagrangian을 살펴보면 다음과 같다.\n",
    "\n",
    "$$ L(\\mathbf{w},b,\\lambda) = \\frac{1}{2} ||\\mathbf{w}||^2 + \\sum_{i=1}^{N} \\lambda_i (1- y_i (\\mathbf{w} \\cdot \\mathbf{x}+b)) $$\n",
    "\n",
    "그러므로 Lagrangian dual은 다음과 같다.\n",
    "\n",
    "$$ g(\\lambda) = inf_{\\mathbf{w},b} \\: L(\\mathbf{w},b,\\lambda) $$\n",
    "\n",
    "$\\mathbf{w},b$에 대한 $L$의 최소값을 구하기 위해 $L$을 $\\mathbf{w},b$에 대해 미분해서 0이 되는 값을 계산하면,\n",
    "\n",
    "- $\\frac{\\partial{L}}{\\partial{b}} = - \\sum_{i=1}^{N} \\lambda_i y_i$\n",
    "\n",
    "- $\\frac{\\partial{L}}{\\partial{\\mathbf{w}}} = \\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{w_1}} \\\\ \\vdots \\\\ \\frac{\\partial{L}}{\\partial{w_n}} \\end{bmatrix} = \\mathbf{w} - \\sum_{i=1}^{N} \\lambda_i y_i \\mathbf{x_i}$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{b}}=0,  \\frac{\\partial{L}}{\\partial{\\mathbf{w}}}=0$ 으로 부터 다음을 얻을 수 있다.\n",
    "\n",
    "$$ \\sum_{i=1}^{N} \\lambda_i y_i=0 \\quad 식(1)\n",
    "\\\\ \\mathbf{w^*} = \\sum_{i=1}^{N} \\lambda_i y_i \\mathbf{x_i} \\quad 식(2)$$\n",
    " \n",
    "그러면 식(1), 식(2)를 Lagrangian에 대입하여 얻은 Lagrangian Dual Function은 다음과 같다.\n",
    "\n",
    "$$ g(\\lambda) = \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j}) - \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j}) - b \\sum_{i=1}^{N} \\lambda_i y_i + \\sum_{i=1}^{N} \\lambda_i\n",
    "\\\\ = \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})  $$\n",
    "\n",
    "그러면 원래 문제(Primal Problem)에 대한 듀얼 문제(Dual Problem)는 다음과 같이 적을 수 있다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad g(\\lambda) = \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})\n",
    "\\\\ & subject \\: to \\quad \\sum_{i=1}^{N} \\lambda_i y_i=0 ~{and}~ \\lambda_i \\ge 0 \n",
    "\\end{aligned}\n",
    "\n",
    "이를 다시 적으면,\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad g(\\boldsymbol{\\lambda}) = \\mathbf{1}^T \\boldsymbol{\\lambda} - \\frac{1}{2} \\boldsymbol{\\lambda}^T Q \\boldsymbol{\\lambda}\n",
    "\\\\ & subject \\: to \\quad \\mathbf{y}^T \\boldsymbol{\\lambda} ~{and}~ \\boldsymbol{\\lambda} \\ge 0\n",
    "\\\\\n",
    "\\\\ & where \\:\\: \\mathbf{1} = [1,1, ... , 1]^T, \\boldsymbol{\\lambda} = [\\lambda_1, \\lambda_2, ..., \\lambda_N]^T,  \\mathbf{y}=[y_1, y_2, ..., y_N]^T\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "일반적으로 Dual Problem의 Optimal 해(denoted by $d^*$)는 Primal Optimal 해(denoted by $p^*$)보다 작다.(under-estimator값이다) 즉, $d^* \\le p^*$\n",
    "\n",
    "Primal Problem이 Convex Problem이고, 부등식 제약조건 중 부등호가 strictly 0보다 크게 하는 $\\mathbf{w}$가 있으므로 이 문제는 Slater's Condition을 만족하므로, Dual Optimal이 Primal Optimal과 같다.\n",
    "\n",
    "정리하면, 주어진 Training data에 대해서, Dual Problem을 푼 다음, ${\\lambda_i}^*$가 결정되면, 분류선의 법선 벡터 $\\mathbf{w}^*$는 다음과 같이 결정 된다.\n",
    "\n",
    "$$ \\mathbf{w}^* = \\sum_{i=1}^{N} {\\lambda_i}^* y_i \\mathbf{x_i}$$\n",
    "\n",
    "다음으로 절편에 해당하는 b값을 계산해야 하는데 b값은 최적화 문제의 KKT(Karush-Kuhn-Tucker) 조건을 이용하여 구할 수 있다.\n",
    "\n",
    "KKT컨디션을 살펴보면,\n",
    "\n",
    "- Primal constraint: $ 1 - y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\le 0, \\: i=1,...,N$\n",
    "- Dual constraint: $\\lambda_i \\ge 0,  \\: i=1,...,N$\n",
    "- Complemetray slackness: $\\lambda_i [1 - y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b)] = 0, \\: i=1,...,N$\n",
    "- Stationary: $ \\nabla_{\\mathbf{w},b} L(\\mathbf{w},b,\\boldsymbol{\\lambda}) = 0 $\n",
    "\n",
    "이 중에 Complementary slackness 조건을 살펴보면, 최적의 평면을 결정하는데 사용되는 $\\lambda_i$중, $\\lambda_i > 0$에 해당하는 데이터가 서포트 벡터가 됨을 알 수 있다.\n",
    "\n",
    "($\\because$ dual problem의 솔루션은 KKT 컨디션을 모두 만족해야 하므로, 서포트 벡터 이외의 데이터에 대해서는, $y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b)] \\ne 1$ 이므로, $\\lambda_i$가 0이 되어야 한다.)\n",
    "\n",
    "서포트 벡터에 대해서는, $ 1 - y_{support}(\\mathbf{w} \\cdot \\mathbf{x_{support}} +b)] = 0$가 성립하므로 $b$ 값을 계산 할 수 있다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
