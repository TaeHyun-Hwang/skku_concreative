{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation)\n",
    "Deep Learning이 활개하는 지금, 고전 알고리즘들이 맥을 못추는 와중에도, 그래도 수학적으로도 훌륭한 알고리즘이 있는데, 그것이 바로 SVM(서포트 벡터 머신)이다.\n",
    "\n",
    "다음의 선형분리가능한(Linearly Seperable) 데이터를 분류하는 경우를 보자.\n",
    "\n",
    "- Figure.1 세가지의 분류 중 어느 분류가 가장 잘 되었다고 볼 수 있는가?\n",
    "![](./image/07/07_01.jpg)\n",
    "이미지 출저:Machine Learning:An Algorithm Perspective 2nd edition_Stephen Marsland\n",
    "\n",
    "아마, 모두 두번째의 경우가 주어진 데이터를 가장 잘 분류했다고 할 것이다. 어떤 기준에서 두번째를 선택하였는지, 왜 두번째가 좋은지를 생각해보자.\n",
    "\n",
    "그 기준을 분류선과 데이터 사이의 여유의 정도(?)를 측정할 수 있다면, 가장 여유롭게 분류하는 게 좋다고 이야기 할 수 있다.\n",
    "\n",
    "그러한 여유의 정도를 마진(Margin)이라고 한다.\n",
    "\n",
    "## Definition) 마진(Margin, Denoted by $M$)\n",
    "\n",
    "The largest radius around the line for which we have a no-man's land where no datapoints lie within the region.\n",
    "\n",
    "분류선을 기준으로, 데이터가 포함되지 않는 영역의 반경을 마진이라고 한다. \n",
    "\n",
    "- Figure.2 Margin, 마진\n",
    "![](./image/07/07_02.png)\n",
    "이미지 출저:Machine Learning:An Algorithm Perspective 2nd edition_Stephen Marsland\n",
    "\n",
    "그래서, 우리는 분류선(3차원 이상에서는 Hyperplane,초평면)을 주어진 데이터에 대해서 마진을 가장 크게 하는 것으로 찾고자 한다.\n",
    "\n",
    "그렇다면 마진을 어떻게 수치화하여 나타낼 것인가를 생각해보자.\n",
    "\n",
    "## Definition) Support Vector\n",
    "\n",
    "The datapoints in each class that lie closest th the classification line\n",
    "\n",
    "주어진 분류선을 기준으로 분류선과 가장 가까운 각 분류에 속한 데이터 점들을 서포트 벡터라고 한다. \n",
    "\n",
    "\n",
    "- Figure.3 Support Vector, 서포트 벡터\n",
    "![](./image/07/07_03.png)\n",
    "이미지 출처:http://www.saedsayad.com/support_vector_machine.htm\n",
    "\n",
    "이러한 서포트 벡터를 이용하여 마진을 수치화 해보자. \n",
    "\n",
    "우선 가장 간단한 두개의 클래스를 가진 N개의 데이터를 분류하는 문제를 생각해보자.\n",
    "\n",
    "2개의 클래스 ('$+$', 'o')에 대해 '$+$'클래스에 있는 데이터에 대해서는 양의 값을,\n",
    "\n",
    "'o' 클래스에 있는 데이터에 대해서는 음의 값을 갖게 하는 분류선 $ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 $을 찾고자 한다.\n",
    "\n",
    "이 때, 주어진 학습데이터의 클래스 중 분류선과 가장 가까이 있는 학습 데이터 즉, 서포트 벡터에 대해서 다음의 식이 성립한다고 하자.\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x_+} + b = 1\n",
    "\\\\ \\mathbf{w} \\cdot \\mathbf{x_o} + b = -1$$\n",
    "- Figure 4.\n",
    "![](./image/07/07_04.png)\n",
    "이미지 출저:J.H. Lee, Lecture note:http://bigs.skku.edu/bigs/menu4/reference.jsp?mode=view&article_no=303114&board_wrapper=%2Fbigs%2Fmenu4%2Freference.jsp&pager.offset=10&board_no=1423\n",
    "\n",
    "이 때, 주어진 분류선의 마진 M은 $(\\mathbf{x_+}-\\mathbf{x_o})$와 분류선의 법선 유닛 벡터($\\frac{\\mathbf{w}}{||\\mathbf{w}||}$)을 이용하여 계산 할 수 있다.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    " 2M \n",
    " & = (\\mathbf{x_+}-\\mathbf{x_o}) \\cdot \\frac{\\mathbf{w}}{||\\mathbf{w}||}\\\\ \n",
    " & = \\frac{1}{||\\mathbf{w}||}(\\mathbf{w} \\cdot \\mathbf{x_+} - \\mathbf{w} \\cdot \\mathbf{x_o} )\\\\\n",
    " & = \\frac{1}{||\\mathbf{w}||}((1-b)-(-1-b))\\\\\n",
    " & = \\frac{2}{||\\mathbf{w}||}\\\\\n",
    "\\therefore \\quad M & = \\frac{1}{||\\mathbf{w}||}\n",
    "\\end{aligned}\n",
    "\n",
    "- Figure 5.\n",
    "![](./image/07/07_05.png)\n",
    "\n",
    "그러므로 우리는 주어진 분류선에 대해 마진 값 M을 계산할 수 있다.\n",
    "\n",
    " \n",
    "## Remark) \n",
    " \n",
    " 위에서 가정한 것처럼 항상 서포트 벡터를 지나는 선의 우변을 +1과 -1로 나타낼 수 있는가?\n",
    " \n",
    " 답을 먼저 말하자면 항상 가능하다.\n",
    " \n",
    " 아래 Figure5에서 (1)의 파란선처럼 분류선의 초기값이 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_0 = 0 $으로 주어졌다고 하자.\n",
    " \n",
    " 이 분류선을 위 아래로 평행이동하면 가장 먼저 데이터와 만나는 두 선 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_0 = C_1, $과 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_0 = C_2 $이 있다. 이 때 만나는 두 선과 만나는 데이터가 서포트벡터이다.\n",
    " \n",
    "  그리고 두 선을 기준으로 분류선의 마진이 최대가 되도록 평행이동 하면 두 선의 가운데에 위치하게 되고 그때 $b_0,C_1,C_2$를 잘 조절하여 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_1 = C_1 $, $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_1 = 0 $, $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_1 = -C $가 되도록 조정할 수 있다.\n",
    "  \n",
    "  그리고 세 선을 모두 스케일링 하여 $ \\mathbf{w} \\cdot \\mathbf{x} + b = 1 $, $ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 $,$ \\mathbf{w} \\cdot \\mathbf{x} + b = -1 $ 으로 만들 수 있다.\n",
    "\n",
    "- Figure 5.\n",
    "![](./image/07/07_06.png)\n",
    "\n",
    "우리는 이 마진 값을 최대로 하는 기울기($\\mathbf{w}$)와 절편($b$)을 찾는 문제가 된다.\n",
    "\n",
    "\\begin{aligned}\n",
    "max\\qquad &\\frac{1}{||\\mathbf{w}||}\n",
    "\\\\  subject \\: to &\\quad (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\{'+'class\\}\n",
    "\\\\\n",
    "& \\quad (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\le 1, \\quad y_i \\in \\{'o'class\\}\n",
    "\\end{aligned}\n",
    "\n",
    "이때,'$+$' 클래스의 타겟 값($y$)을 $+1$, 'o' 클래스의 타겟 값을 $-1$이라고 한다면, \n",
    "\n",
    "위의 제약조건을 다음과 같이 하나로 정리하여 나타낼 수 있다.\n",
    "\n",
    "$$ y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}$$\n",
    "\n",
    "이를 정리하면, SVM은 부등식 제약조건이 있는 목적함수의 최대값을 찾는 최적화 문제로 표현할 수 있다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad \\frac{1}{||\\mathbf{w}||}\n",
    "\\\\ & subject \\: to \\quad y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "여기서 $\\frac{1}{||\\mathbf{w}||}$을 최대로 하는 것은 $||\\mathbf{w}||$의 값을 최소로 하는 것과 동치이므로, 최소값을 찾는 문제로 적으면,\n",
    "\n",
    "\\begin{aligned}\n",
    "&min \\quad \\frac{1}{2} ||\\mathbf{w}||^2\n",
    "\\\\ & subject \\: to \\quad y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "이제 우리는 convex optimization 문제를 얻어냈다.\n",
    "\n",
    "이 문제의 Lagrangian을 살펴보면 다음과 같다.\n",
    "\n",
    "$$ L(\\mathbf{w},b,\\lambda) = \\frac{1}{2} ||\\mathbf{w}||^2 + \\sum_{i=1}^{N} \\lambda_i (1- y_i (\\mathbf{w} \\cdot \\mathbf{x_i}+b)) $$\n",
    "\n",
    "그러므로 Lagrangian dual은 다음과 같다.\n",
    "\n",
    "$$ g(\\lambda) = inf_{\\mathbf{w},b} \\: L(\\mathbf{w},b,\\lambda) $$\n",
    "\n",
    "$\\mathbf{w},b$에 대한 $L$의 최소값을 구하기 위해 $L$을 $\\mathbf{w},b$에 대해 미분해서 0이 되는 값을 계산하면,\n",
    "\n",
    "- $\\frac{\\partial{L}}{\\partial{b}} = - \\sum_{i=1}^{N} \\lambda_i y_i$\n",
    "\n",
    "- $\\frac{\\partial{L}}{\\partial{\\mathbf{w}}} = \\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{w_1}} \\\\ \\vdots \\\\ \\frac{\\partial{L}}{\\partial{w_n}} \\end{bmatrix} = \\mathbf{w} - \\sum_{i=1}^{N} \\lambda_i y_i \\mathbf{x_i}$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{b}}=0,  \\frac{\\partial{L}}{\\partial{\\mathbf{w}}}=0$ 으로 부터 다음을 얻을 수 있다.\n",
    "\n",
    "$$ \\sum_{i=1}^{N} \\lambda_i y_i=0 \\quad 식(1)\n",
    "\\\\ \\mathbf{w^*} = \\sum_{i=1}^{N} \\lambda_i y_i \\mathbf{x_i} \\quad 식(2)$$\n",
    " \n",
    "그러면 식(1), 식(2)를 Lagrangian에 대입하여 얻은 Lagrangian Dual Function은 다음과 같다.\n",
    "\n",
    "$$ g(\\lambda) = \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j}) - \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j}) - b \\sum_{i=1}^{N} \\lambda_i y_i + \\sum_{i=1}^{N} \\lambda_i\n",
    "\\\\ = \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})  $$\n",
    "\n",
    "그러면 원래 문제(Primal Problem)에 대한 듀얼 문제(Dual Problem)는 다음과 같이 적을 수 있다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad g(\\lambda) = \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})\n",
    "\\\\ & subject \\: to \\quad \\sum_{i=1}^{N} \\lambda_i y_i=0 ~{and}~ \\lambda_i \\ge 0 \n",
    "\\end{aligned}\n",
    "\n",
    "이를 다시 적으면,\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad g(\\boldsymbol{\\lambda}) = \\mathbf{1}^T \\boldsymbol{\\lambda} - \\frac{1}{2} \\boldsymbol{\\lambda}^T Q \\boldsymbol{\\lambda}\n",
    "\\\\ & subject \\: to \\quad \\mathbf{y}^T \\boldsymbol{\\lambda} ~{and}~ \\boldsymbol{\\lambda} \\ge 0\n",
    "\\\\\n",
    "\\\\ & where \\:\\: \\mathbf{1} = [1,1, ... , 1]^T, \\boldsymbol{\\lambda} = [\\lambda_1, \\lambda_2, ..., \\lambda_N]^T,  \\mathbf{y}=[y_1, y_2, ..., y_N]^T, Q=[Q_{i,j}] \\ with \\ Q_{ij}=y_iy_jx_i\\cdot x_j\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "일반적으로 Dual Problem의 Optimal 해(denoted by $d^*$)는 Primal Optimal 해(denoted by $p^*$)보다 작다.(under-estimator값이다) 즉, $d^* \\le p^*$\n",
    "\n",
    "그러나 Primal Problem이 Convex Problem이고, 부등식 제약조건이 모두 선형 함수이므로 이 문제는 Slater's Condition을 만족한다. 그러므로 Dual Optimal이 Primal Optimal과 같다.\n",
    "\n",
    "정리하면, 주어진 Training data에 대해서, Dual Problem을 푼 다음, ${\\lambda_i}^*$가 결정되면, 분류선의 법선 벡터 $\\mathbf{w}^*$는 다음과 같이 결정 된다.\n",
    "\n",
    "$$ \\mathbf{w}^* = \\sum_{i=1}^{N} {\\lambda_i}^* y_i \\mathbf{x_i}$$\n",
    "\n",
    "다음으로 절편에 해당하는 b값을 계산해야 하는데 b값은 최적화 문제의 KKT(Karush-Kuhn-Tucker) 조건을 이용하여 구할 수 있다.\n",
    "\n",
    "KKT컨디션을 살펴보면,\n",
    "\n",
    "- Primal constraint: $ 1 - y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\le 0, \\: i=1,...,N$\n",
    "- Dual constraint: $\\lambda_i \\ge 0,  \\: i=1,...,N$\n",
    "- Complemetray slackness: $\\lambda_i [1 - y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b)] = 0, \\: i=1,...,N$\n",
    "- Stationary: $ \\nabla_{\\mathbf{w},b} L(\\mathbf{w},b,\\boldsymbol{\\lambda}) = 0 $\n",
    "\n",
    "이 중에 Complementary slackness 조건을 살펴보면, 최적의 평면을 결정하는데 사용되는 $\\lambda_i$중, $\\lambda_i > 0$에 해당하는 데이터가 서포트 벡터가 됨을 알 수 있다.\n",
    "\n",
    "($\\because$ dual problem의 솔루션은 KKT 컨디션을 모두 만족해야 하므로, 서포트 벡터 이외의 데이터에 대해서는, $y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b)] \\ne 1$ 이므로, $\\lambda_i$가 0이 되어야 한다.)\n",
    "\n",
    "서포트 벡터에 대해서는, $ 1 - y_{support}(\\mathbf{w} \\cdot \\mathbf{x_{support}} +b)] = 0$가 성립하므로 $b$ 값을 계산 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kernel-SVM\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Kernel-SVM은 기본적으로 SVM과 같이 마진을 최대회하는 초평면을 찾는 기법이다. 단지, 원래 데이터가 있는 Input space에서 선형으로 분류하기 힘든 경우 선형분류가 가능한 차원으로 매핑한 뒤 분류하는 초평면을 찾는 기법이다.\n",
    "\n",
    "\n",
    "선형분류가 안되는 대표적인 예로 XOR문제가 있다. XOR은 인풋이 서로 다르면 1, 아니면 0을 갖는 함수이다. 이때 직선으로 0과 1을 분류하는 것은 불가능하다. \n",
    "\n",
    "- Figure 7. XOR 문제\n",
    "![](./image/07/07_08.png)\n",
    "이미지 출처: https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/\n",
    "\n",
    "\n",
    "## Definition) Feature map  $\\boldsymbol{\\Phi}$\n",
    "\n",
    "The feature map $x\\rightarrow\\boldsymbol{\\Phi}$ is mapping the $x\\in X$ into feature space of higher dimensions $\\boldsymbol{\\Phi}(x)$ in which the classes can be linearly separated where $X$ is the input space.\n",
    "\n",
    "Featrue map는 위의 XOR문제처럼 기존의 Input space에서 데이터가 선형으로 분류되지 않을때 고차원의 Feature space로 매핑시켜주는 함수이다. 앞의 XOR 문제에서는 Featrue map으로 \n",
    "$$\\boldsymbol{\\Phi}:(x_1,x_2)\\rightarrow(x_1^2,x_2^2,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2}x_1x_2,1)$$\n",
    "을 이용하여 2차원 데이터를 6차원으로 매핑시켜 선형분리를 할 수 있게 된다.\n",
    "\n",
    "- Figure 8. XOR Feature map\n",
    "![](./image/07/07_09.png)\n",
    "이미지 출처:https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/\n",
    "\n",
    "이러한 Feature map을 이용하면 기존의 최적화 문제가 다음과 같이 바뀐다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad g(\\lambda) = \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j))\n",
    "\\\\ & subject \\: to \\quad \\sum_{i=1}^{N} \\lambda_i y_i=0 ~{and}~ \\lambda_i \\ge 0 \n",
    "\\end{aligned}\n",
    "\n",
    "##  Kernel\n",
    "\n",
    "Feature map를 이용하면 기존의 선형분리가 안되는 문제를 해결할 수는 있지만 매핑을 하고 내적도 해야하기 때문에 계산량이 매우 많아진다. 그래서 대용량 데이터에서 실제로 사용하기는 매우 힘들다.\n",
    "\n",
    "이러한 문제를 해결하기 위해 나온 방법이 Kernel이다. 만약 어떤 함수 $K$가 존재해서 다음의 식을 만족시킨다고 하자.\n",
    "\n",
    "$$K(x_i,x_j)=\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j)$$\n",
    "\n",
    "K함수를 알고 있다면 직접 내적을 할 필요 없이 함수에 대입만 하면 되므로 계산이 매우 빨라진다. 이러한 함수 K를 커널이라고 한다.\n",
    "\n",
    "그러면 이런 커널을 어떻게 구해야 할까?\n",
    "\n",
    "## Mercer's Theorem\n",
    "\n",
    "Mercer's theorem에서 커널이 되기위한 필요충분 조건을 알 수 있다.\n",
    "\n",
    "- Figure 9. Mercer's Theoren\n",
    "![](./image/07/07_10.png)\n",
    "이미지 출처:https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf\n",
    "\n",
    "## Some Kernels\n",
    "\n",
    "잘 알려진 4개의 커널 함수는 아래와 같다.\n",
    "\n",
    "- Figure 10. Some Kernels\n",
    "![](./image/07/07_11.png)\n",
    "이미지 출처:https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
