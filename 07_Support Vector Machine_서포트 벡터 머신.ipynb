{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard SVM\n",
    "\n",
    "## Motivation)\n",
    "Deep Learning이 활개하는 지금, 고전 알고리즘들이 맥을 못추는 와중에도, 그래도 수학적으로도 훌륭한 알고리즘이 있는데, 그것이 바로 SVM(서포트 벡터 머신)이다.\n",
    "\n",
    "다음의 선형분리가능한(Linearly Seperable) 데이터를 분류하는 경우를 보자.\n",
    "\n",
    "- Figure.1 세가지의 분류 중 어느 분류가 가장 잘 되었다고 볼 수 있는가?\n",
    "![](./image/07/07_01.jpg)\n",
    "이미지 출저:Machine Learning:An Algorithm Perspective 2nd edition_Stephen Marsland\n",
    "\n",
    "아마, 모두 두번째의 경우가 주어진 데이터를 가장 잘 분류했다고 할 것이다. 어떤 기준에서 두번째를 선택하였는지, 왜 두번째가 좋은지를 생각해보자.\n",
    "\n",
    "그 기준을 분류선과 데이터 사이의 여유의 정도(?)를 측정할 수 있다면, 가장 여유롭게 분류하는 게 좋다고 이야기 할 수 있다.\n",
    "\n",
    "그러한 여유의 정도를 마진(Margin)이라고 한다.\n",
    "\n",
    "## Definition) 마진(Margin, Denoted by $M$)\n",
    "\n",
    "The largest radius around the line for which we have a no-man's land where no datapoints lie within the region.\n",
    "\n",
    "분류선을 기준으로, 데이터가 포함되지 않는 영역의 반경을 마진이라고 한다. \n",
    "\n",
    "- Figure.2 Margin, 마진\n",
    "![](./image/07/07_02.png)\n",
    "이미지 출저:Machine Learning:An Algorithm Perspective 2nd edition_Stephen Marsland\n",
    "\n",
    "그래서, 우리는 분류선(3차원 이상에서는 Hyperplane,초평면)을 주어진 데이터에 대해서 마진을 가장 크게 하는 것으로 찾고자 한다.\n",
    "\n",
    "그렇다면 마진을 어떻게 수치화하여 나타낼 것인가를 생각해보자.\n",
    "\n",
    "## Definition) Support Vector\n",
    "\n",
    "The datapoints in each class that lie closest th the classification line\n",
    "\n",
    "주어진 분류선을 기준으로 분류선과 가장 가까운 각 분류에 속한 데이터 점들을 서포트 벡터라고 한다. \n",
    "\n",
    "\n",
    "- Figure.3 Support Vector, 서포트 벡터\n",
    "![](./image/07/07_03.png)\n",
    "이미지 출처:http://www.saedsayad.com/support_vector_machine.htm\n",
    "\n",
    "이러한 서포트 벡터를 이용하여 마진을 수치화 해보자. \n",
    "\n",
    "우선 가장 간단한 두개의 클래스를 가진 N개의 데이터를 분류하는 문제를 생각해보자.\n",
    "\n",
    "2개의 클래스 ('$+$', 'o')에 대해 '$+$'클래스에 있는 데이터에 대해서는 양의 값을,\n",
    "\n",
    "'o' 클래스에 있는 데이터에 대해서는 음의 값을 갖게 하는 분류선 $ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 $을 찾고자 한다.\n",
    "\n",
    "이 때, 주어진 학습데이터의 클래스 중 분류선과 가장 가까이 있는 학습 데이터 즉, 서포트 벡터에 대해서 다음의 식이 성립한다고 하자.\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x_+} + b = 1\n",
    "\\\\ \\mathbf{w} \\cdot \\mathbf{x_o} + b = -1$$\n",
    "- Figure 4.\n",
    "![](./image/07/07_04.png)\n",
    "이미지 출저:J.H. Lee, Lecture note:http://bigs.skku.edu/bigs/menu4/reference.jsp?mode=view&article_no=303114&board_wrapper=%2Fbigs%2Fmenu4%2Freference.jsp&pager.offset=10&board_no=1423\n",
    "\n",
    "이 때, 주어진 분류선의 마진 M은 $(\\mathbf{x_+}-\\mathbf{x_o})$와 분류선의 법선 유닛 벡터($\\frac{\\mathbf{w}}{||\\mathbf{w}||}$)을 이용하여 계산 할 수 있다.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    " 2M \n",
    " & = (\\mathbf{x_+}-\\mathbf{x_o}) \\cdot \\frac{\\mathbf{w}}{||\\mathbf{w}||}\\\\ \n",
    " & = \\frac{1}{||\\mathbf{w}||}(\\mathbf{w} \\cdot \\mathbf{x_+} - \\mathbf{w} \\cdot \\mathbf{x_o} )\\\\\n",
    " & = \\frac{1}{||\\mathbf{w}||}((1-b)-(-1-b))\\\\\n",
    " & = \\frac{2}{||\\mathbf{w}||}\\\\\n",
    "\\therefore \\quad M & = \\frac{1}{||\\mathbf{w}||}\n",
    "\\end{aligned}\n",
    "\n",
    "- Figure 5.\n",
    "![](./image/07/07_05.png)\n",
    "\n",
    "그러므로 우리는 주어진 분류선에 대해 마진 값 M을 계산할 수 있다.\n",
    "\n",
    " \n",
    "## Remark) \n",
    " \n",
    " 위에서 가정한 것처럼 항상 서포트 벡터를 지나는 선의 우변을 +1과 -1로 나타낼 수 있는가?\n",
    " \n",
    " 답을 먼저 말하자면 항상 가능하다.\n",
    " \n",
    " 아래 Figure5에서 (1)의 파란선처럼 분류선의 초기값이 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_0 = 0 $으로 주어졌다고 하자.\n",
    " \n",
    " 이 분류선을 위 아래로 평행이동하면 가장 먼저 데이터와 만나는 두 선 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_0 = C_1, $과 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_0 = C_2 $이 있다. 이 때 만나는 두 선과 만나는 데이터가 서포트벡터이다.\n",
    " \n",
    "  그리고 두 선을 기준으로 분류선의 마진이 최대가 되도록 평행이동 하면 두 선의 가운데에 위치하게 되고 그때 $b_0,C_1,C_2$를 잘 조절하여 $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_1 = C_1 $, $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_1 = 0 $, $ \\mathbf{w_0} \\cdot \\mathbf{x} + b_1 = -C $가 되도록 조정할 수 있다.\n",
    "  \n",
    "  그리고 세 선을 모두 스케일링 하여 $ \\mathbf{w} \\cdot \\mathbf{x} + b = 1 $, $ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 $,$ \\mathbf{w} \\cdot \\mathbf{x} + b = -1 $ 으로 만들 수 있다.\n",
    "\n",
    "- Figure 5.\n",
    "![](./image/07/07_06.png)\n",
    "\n",
    "우리는 이 마진 값을 최대로 하는 기울기($\\mathbf{w}$)와 절편($b$)을 찾는 문제가 된다.\n",
    "\n",
    "\\begin{aligned}\n",
    "max\\qquad &\\frac{1}{||\\mathbf{w}||}\n",
    "\\\\  subject \\: to &\\quad (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\{'+'class\\}\n",
    "\\\\\n",
    "& \\quad (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\le 1, \\quad y_i \\in \\{'o'class\\}\n",
    "\\end{aligned}\n",
    "\n",
    "이때,'$+$' 클래스의 타겟 값($y$)을 $+1$, 'o' 클래스의 타겟 값을 $-1$이라고 한다면, \n",
    "\n",
    "위의 제약조건을 다음과 같이 하나로 정리하여 나타낼 수 있다.\n",
    "\n",
    "$$ y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}$$\n",
    "\n",
    "이를 정리하면, SVM은 부등식 제약조건이 있는 목적함수의 최대값을 찾는 최적화 문제로 표현할 수 있다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad \\frac{1}{||\\mathbf{w}||}\n",
    "\\\\ & subject \\: to \\quad y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "여기서 $\\frac{1}{||\\mathbf{w}||}$을 최대로 하는 것은 $||\\mathbf{w}||$의 값을 최소로 하는 것과 동치이므로, 최소값을 찾는 문제로 적으면,\n",
    "\n",
    "\\begin{aligned}\n",
    "&min \\quad \\frac{1}{2} ||\\mathbf{w}||^2\n",
    "\\\\ & subject \\: to \\quad y_{i} (\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "이제 우리는 convex optimization 문제를 얻어냈다.\n",
    "\n",
    "이 문제의 Lagrangian을 살펴보면 다음과 같다.\n",
    "\n",
    "$$ L(\\mathbf{w},b,\\lambda) = \\frac{1}{2} ||\\mathbf{w}||^2 + \\sum_{i=1}^{N} \\lambda_i (1- y_i (\\mathbf{w} \\cdot \\mathbf{x_i}+b)) $$\n",
    "\n",
    "그러므로 Lagrangian dual은 다음과 같다.\n",
    "\n",
    "$$ g(\\lambda) = inf_{\\mathbf{w},b} \\: L(\\mathbf{w},b,\\lambda) $$\n",
    "\n",
    "$\\mathbf{w},b$에 대한 $L$의 최소값을 구하기 위해 $L$을 $\\mathbf{w},b$에 대해 미분해서 0이 되는 값을 계산하면,\n",
    "\n",
    "- $\\frac{\\partial{L}}{\\partial{b}} = - \\sum_{i=1}^{N} \\lambda_i y_i$\n",
    "\n",
    "- $\\frac{\\partial{L}}{\\partial{\\mathbf{w}}} = \\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{w_1}} \\\\ \\vdots \\\\ \\frac{\\partial{L}}{\\partial{w_n}} \\end{bmatrix} = \\mathbf{w} - \\sum_{i=1}^{N} \\lambda_i y_i \\mathbf{x_i}$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{b}}=0,  \\frac{\\partial{L}}{\\partial{\\mathbf{w}}}=0$ 으로 부터 다음을 얻을 수 있다.\n",
    "\n",
    "$$ \\sum_{i=1}^{N} \\lambda_i y_i=0 \\quad 식(1)\n",
    "\\\\ \\mathbf{w^*} = \\sum_{i=1}^{N} \\lambda_i y_i \\mathbf{x_i} \\quad 식(2)$$\n",
    " \n",
    "그러면 식(1), 식(2)를 Lagrangian에 대입하여 얻은 Lagrangian Dual Function은 다음과 같다.\n",
    "\n",
    "$$ g(\\lambda) = \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j}) - \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j}) - b \\sum_{i=1}^{N} \\lambda_i y_i + \\sum_{i=1}^{N} \\lambda_i\n",
    "\\\\ = \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})  $$\n",
    "\n",
    "그러면 원래 문제(Primal Problem)에 대한 듀얼 문제(Dual Problem)는 다음과 같이 적을 수 있다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad g(\\lambda) = \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})\n",
    "\\\\ & subject \\: to \\quad \\sum_{i=1}^{N} \\lambda_i y_i=0 ~{and}~ \\lambda_i \\ge 0 \n",
    "\\end{aligned}\n",
    "\n",
    "이를 다시 적으면,\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad g(\\boldsymbol{\\lambda}) = \\mathbf{1}^T \\boldsymbol{\\lambda} - \\frac{1}{2} \\boldsymbol{\\lambda}^T Q \\boldsymbol{\\lambda}\n",
    "\\\\ & subject \\: to \\quad \\mathbf{y}^T \\boldsymbol{\\lambda} ~{and}~ \\boldsymbol{\\lambda} \\ge 0\n",
    "\\\\\n",
    "\\\\ & where \\:\\: \\mathbf{1} = [1,1, ... , 1]^T, \\boldsymbol{\\lambda} = [\\lambda_1, \\lambda_2, ..., \\lambda_N]^T,  \\mathbf{y}=[y_1, y_2, ..., y_N]^T, Q=[Q_{i,j}] \\ with \\ Q_{ij}=y_iy_jx_i\\cdot x_j\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "일반적으로 Dual Problem의 Optimal 해(denoted by $d^*$)는 Primal Optimal 해(denoted by $p^*$)보다 작다.(under-estimator값이다) 즉, $d^* \\le p^*$\n",
    "\n",
    "그러나 Primal Problem이 Convex Problem이고, 부등식 제약조건이 모두 선형 함수이므로 이 문제는 Slater's Condition을 만족한다. 그러므로 Dual Optimal이 Primal Optimal과 같다.\n",
    "\n",
    "정리하면, 주어진 Training data에 대해서, Dual Problem을 푼 다음, ${\\lambda_i}^*$가 결정되면, 분류선의 법선 벡터 $\\mathbf{w}^*$는 다음과 같이 결정 된다.\n",
    "\n",
    "$$ \\mathbf{w}^* = \\sum_{i=1}^{N} {\\lambda_i}^* y_i \\mathbf{x_i}$$\n",
    "\n",
    "다음으로 절편에 해당하는 b값을 계산해야 하는데 b값은 최적화 문제의 KKT(Karush-Kuhn-Tucker) 조건을 이용하여 구할 수 있다.\n",
    "\n",
    "KKT컨디션을 살펴보면,\n",
    "\n",
    "- Primal constraint: $ 1 - y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b) \\le 0, \\: i=1,...,N$\n",
    "- Dual constraint: $\\lambda_i \\ge 0,  \\: i=1,...,N$\n",
    "- Complemetray slackness: $\\lambda_i [1 - y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b)] = 0, \\: i=1,...,N$\n",
    "- Stationary: $ \\nabla_{\\mathbf{w},b} L(\\mathbf{w},b,\\boldsymbol{\\lambda}) = 0 $\n",
    "\n",
    "이 중에 Complementary slackness 조건을 살펴보면, 최적의 평면을 결정하는데 사용되는 $\\lambda_i$중, $\\lambda_i > 0$에 해당하는 데이터가 서포트 벡터가 됨을 알 수 있다.\n",
    "\n",
    "($\\because$ dual problem의 솔루션은 KKT 컨디션을 모두 만족해야 하므로, 서포트 벡터 이외의 데이터에 대해서는, $y_{i}(\\mathbf{w} \\cdot \\mathbf{x_i} +b)] \\ne 1$ 이므로, $\\lambda_i$가 0이 되어야 한다.)\n",
    "\n",
    "서포트 벡터에 대해서는, $ 1 - y_{support}(\\mathbf{w} \\cdot \\mathbf{x_{support}} +b)] = 0$가 성립하므로 $b$ 값을 계산 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kernel-SVM\n",
    "\n",
    "## Motivation)\n",
    "지금까지는 선형으로 분리 가능한 데이터들에 대해서, 최적의 초평면을 찾는 일을 했었다. 하지만 항상 선형으로 분리가능한 데이터만 있지는 않다. 그렇다면 선형으로 분리할 수 없는 데이터에 대해서는 어떻게 할 수 있는지 생각해보자.\n",
    "\n",
    "먼저 다음과 같이 XOR 문제를 생각해보자.\n",
    "\n",
    "- Figure 7. XOR 문제: Input의 x,y값의 부호가 같으면 1, 다르면 0을 갖는 문제. 선형으로 분류 할 수 없다.\n",
    "![](./image/07/07_08.png)\n",
    "이미지 출처: https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/\n",
    "\n",
    "이 때, 할 수 있는 방법이 원래 데이터를 선형으로 분류가 가능한 다른 차원으로 이동한 후 새로운 공간에서 선형으로 분류하는 초평면을 찾는 것이다.\n",
    "\n",
    "그렇다면, 원래 데이터를 선형으로 분류가 가능한 다른 차원으로 어떻게 이동할 것인가?\n",
    "\n",
    "## Feature map  $\\boldsymbol{\\Phi}$\n",
    "\n",
    "XOR문제에서 다음과 같은 함수를 생각해보자.\n",
    "\n",
    "$$\\boldsymbol{\\Phi}:(x_1,x_2)\\longmapsto(x_1^2,x_2^2,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2}x_1x_2,1)$$\n",
    "\n",
    "$\\boldsymbol{\\Phi}$를 이용하여 주어진 2차원의 데이터를 6차원으로 보내면, 다음과 같이 선형분리가 가능해진다.\n",
    "\n",
    "- Figure 8. XOR Feature map\n",
    "![](./image/07/07_09.png)\n",
    "이미지 출처:https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/\n",
    "\n",
    "또 다른 예제를 살펴보자.\n",
    "\n",
    "다음과 같이 1차원의 데이터가 주어져 있다고 하자.\n",
    "\n",
    "- Figure 9. 1차원상의 선형분리 불가능한 데이터\n",
    "![](./image/07/07_12.png)\n",
    "\n",
    "이제 이 데이터에 대해서 다음과 같은 함수를 생각해보자.\n",
    "\n",
    "$$\\boldsymbol{\\Phi}:x\\longmapsto(x,x^2)$$\n",
    "\n",
    "위와 같은 함수를 이용하여 데이터를 표현해보면, 다음과 같이 선형분리가 가능해진다.\n",
    "\n",
    "- Figure 10. $\\boldsymbol{\\Phi}$함수를 이용한 새로운 공간에서의 데이터\n",
    "![](./image/07/07_13.png)\n",
    "\n",
    "이렇게, 주어진 데이터에 대해서 선형분리가 가능한 새로운 공간으로 보내는 함수를 Feature map이라고 한다.\n",
    "\n",
    "## Definition) Feature Mapping\n",
    "For given $\\mathbf{x} \\in \\mathbb{R}^n$ the map $\\boldsymbol{\\Phi}: \\mathbf{R}^n \\longrightarrow \\mathbf{R}^d$ such that $\\boldsymbol{\\Phi}(x)$ is linear separable in its space is called a feature mapping\n",
    "\n",
    "주어진 데이터에 대해 선형분리 가능한 공간으로 보내주는 함수$\\boldsymbol{\\Phi}$를 Feature Mapping이라고 한다. 그리고 Feature map으로 보내진 데이터가 있는 공간을 Feature Space라고 한다.\n",
    "\n",
    "그러면, 이제 우리는 원래 데이터가 주어진 공간에서가 아닌, Feature space에서 데이터들을 분리하는 초평면을 찾는 문제가 된다.\n",
    "\n",
    "단지 $\\mathbf{x}$에서 $\\boldsymbol{\\Phi}(x)$를 이용한 것일 뿐 다른 조건은 그대로 이므로, 이러한 Feature map을 이용하면 기존의 최적화 문제가 다음과 같이 바뀐다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&min \\quad \\frac{1}{2} ||\\mathbf{w}||^2 \\quad {where} \\: \\mathbf{w} \\in \\mathbf{R}^d\n",
    "\\\\ & subject \\: to \\quad y_{i} (\\mathbf{w} \\cdot \\boldsymbol{\\Phi}(\\mathbf{x_i}) +b) \\ge 1, \\quad y_i \\in \\left\\{ 1, -1 \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "이의 Dual Problem은 다음과 같다.\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j))\n",
    "\\\\ & subject \\: to \\quad \\sum_{i=1}^{N} \\lambda_i y_i=0 ~{and}~ \\lambda_i \\ge 0 \n",
    "\\end{aligned}\n",
    "\n",
    "그런데, 이렇게 바뀐 문제는 상당히 풀기가 어려워 진다. 그 이유는 먼저, Feature Mapping은 기본적으로 원래 데이터의 차원보다 더 높은 차원으로 데이터를 보내기 때문에 계산해야할 양이 많아지기 때문이다.\n",
    "\n",
    "하지만 여기서 조금만 생각해보면 우리는 굳이 Feature Mapping을 구할 필요가 없다는 사실을 알 수 있다.\n",
    "\n",
    "그 이유는 Feature Mapping을 통해 바뀐 Dual Problem에서 Feature Mapping인 $\\boldsymbol{\\Phi}$가 들어간 부분을 살펴보면, $(\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j))$ 즉, $\\boldsymbol{\\Phi}$함수의 내적값으로 나타나져있다.\n",
    "\n",
    "그래서 우리는 $\\boldsymbol{\\Phi}$함수 말고 $\\boldsymbol{\\Phi}$를 통해 변환된 데이터들의 내적값에만 관심이 있기 때문에,\n",
    "\n",
    "만약 어떤 $\\boldsymbol{\\Phi}$에 대해 다음과 같은 조건을 만족하는 함수 $K$가 존재한다면,\n",
    "\n",
    "$$ K(\\mathbf{x_i}, \\mathbf{x_j}) = (\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j))$$\n",
    "\n",
    "우리는 굳이 $\\boldsymbol{\\Phi}$를 계산하지 않아도 되므로 계산량이 상당히 줄어들게 된다.\n",
    "\n",
    "이러한 조건을 만족시키는 함수 $K$를 우리는 kernel이라고 부른다.\n",
    "\n",
    "##  Kernel\n",
    "\n",
    "Feature map를 이용하여 기존의 선형분리가 안되는 문제를 선형분리가 가능한 공간, Feautre Space로 보내서 문제를 해결할 수는 있지만, Feature mapping 자체 계산과 계산 후 내적을 해야하기 때문에 대용량의 데이터에 대해서 계산량이 매우 많아진다.\n",
    "\n",
    "이러한 문제를 해결하기 위해 원래 데이터를 Feature Mapping후의 내적값에 대응 시키는 함수를 Kernel이라고 한다고 앞서 언급했다.\n",
    "\n",
    "$$ K(\\mathbf{x_i}, \\mathbf{x_j}) = (\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j))$$\n",
    "\n",
    "그래서 우리는 Feature Mapping을 찾는 대신에, Feauture Mapping 후의 내적값으로 대응되는 Kernel함수를 이용하여 Feature Space에서의 최적화를 원래 데이터의 공간에서의 계산을 이용해서 하려고 한다.\n",
    "\n",
    "## Example)\n",
    "다음과 같이 2차원의 데이터가 있다고 하자.\n",
    "![](./image/07/07_14.png)\n",
    "\n",
    "선형분리 불가능한 이 데이터를 다음과 같은 Feature Mapping을 이용하여,\n",
    "\n",
    "$$\\boldsymbol{\\Phi}:(x_1,x_2)\\longmapsto(x_1^2,x_2^2,\\sqrt{2}x_1 x_2)$$\n",
    "\n",
    "3차원에 나타내면, 다음과 같이 선형 분리 가능한 데이터가 된다.\n",
    "![](./image/07/07_15.png)\n",
    "\n",
    "이 때, $\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j)$을 계산 해보면,\n",
    "\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\Phi}(x) \\cdot \\boldsymbol{\\Phi}(y) &= (x_1^2, x_2^2, \\sqrt{2} x_1 x_2) \\cdot (y_1^2, y_2^2, \\sqrt{2} y_1 y_2)\n",
    "\\\\ &= x_1^2 y_1^2 + x_2^2 y_2^2 +2 x_1 x_2 y_1 y_2 \n",
    "\\\\ &= (x_1 y_1 + x_2 y_2 )^2\n",
    "\\\\ &= (\\mathbf{x} \\cdot \\mathbf{y})^2\n",
    "\\end{aligned}\n",
    "\n",
    "그래서, 우리는 Kernel $K$를 다음과 같이 정의할 수 있다.\n",
    "\n",
    "$$ K(\\mathbf{x_i}, \\mathbf{x_j}) = (\\mathbf{x_i} \\cdot \\mathbf{x_j})^2 $$\n",
    "\n",
    "이렇게 정의가 된 Kernel을 이용하면 $\\boldsymbol{\\Phi}$를 계산하지 않아도, Feature Space에서의 최적화를 원래 데이터 공간에서의 값만으로도 계산이 가능하다.\n",
    "\n",
    "그렇다면 Kernel은 어떻게 정의할 수 있고 어떤 함수를 Kernel로 이용할 수 있는가?\n",
    "\n",
    "먼저 Feature Mapping과 Kernel을 다시 한 번 정리하고 가자.\n",
    "\n",
    "## Remark) Properites of Kernels\n",
    "- 1.각각의 Kernel은 하나 이상의 Feature Mapping과 대응이 될 수있다.\n",
    "\n",
    "예를 들어서 앞서 정의한 Kernel $K(\\mathbf{x_i}, \\mathbf{x_j}) = (\\mathbf{x_i} \\cdot \\mathbf{x_j})^2$ 을 살펴보자.\n",
    "\n",
    "이 Kernel은 $\\boldsymbol{\\Phi}:(x_1,x_2)\\longmapsto(x_1^2,x_2^2,\\sqrt{2}x_1 x_2)$ 함수와 대응이 됨을 우리는 앞서 확인 했다.\n",
    "\n",
    "그런데, 다음 함수를 생각해보자.\n",
    "\n",
    "$$\\boldsymbol{\\Phi'}:(x_1,x_2)\\longmapsto(x_1^2,x_1 x_2, x_1 x_2,x_2^2)$$\n",
    "\n",
    "그러면,\n",
    "\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\Phi'}(x) \\cdot \\boldsymbol{\\Phi'}(y) &= (x_1^2, x_1 x_2, x_1 x_2, x_2^2) \\cdot (y_1^2, y_1 y_2, y_1 y_2, y_2^2)\n",
    "\\\\ &= x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2 \n",
    "\\\\ &= (x_1 y_1 + x_2 y_2 )^2\n",
    "\\\\ &= (\\mathbf{x} \\cdot \\mathbf{y})^2\n",
    "\\end{aligned}\n",
    "\n",
    "즉, 앞서 정의한 Kernel은 또 다른 Feature Mapping으로도 정의 될 수 있다.\n",
    "\n",
    "- 2.Feature Mapping $\\boldsymbol{\\Phi}$는 Data Space(데이터가 있는 공간)에서 높은 차원의 공간(Feature Space)로 보내는 함수이다.\n",
    "- 3.Kernel은 두개의 데이터를 받아서, 각 데이터와 대응 되는 Feature space에서의 점 사이의 내적과 대응 시키는 함수이다.\n",
    "$$ K: X \\times X \\rightarrow \\mathbb{R},\\quad  K(\\mathbf{x},\\mathbf{y}) = \\boldsymbol{\\Phi}(\\mathbf{x}) \\cdot \\boldsymbol{\\Phi}(\\mathbf{y}))$$\n",
    "- 4.당연이 Feature space는 dot product(내적)이 정의된 공간이어야 한다.\n",
    "\n",
    "그러면 이제 어떤 함수들이 Kernel로 사용될 수 있는지 살펴보자.\n",
    "\n",
    "Kernel이 되기 위한 필요충분조건은 다음 Mercer's Theorem으로 설명 된다.\n",
    "\n",
    "## Mercer's Theorem\n",
    "T.F.A.E (다음의 것들이 서로 동치이다)\n",
    "\n",
    "1. For K, there exist a mapping $\\boldsymbol{\\Phi}$ and an expansion $K(\\mathbf{x},\\mathbf{y}) = \\boldsymbol{\\Phi}(\\mathbf{x}) \\cdot \\boldsymbol{\\Phi}(\\mathbf{y})$: 즉, K가 우리가 아는 Kernel 함수가 된다\n",
    "\n",
    "2. For any $g(\\mathbf{x})$ such that $\\int g(\\mathbf{x})^2 d\\mathbf{x} < \\infty$,\n",
    "$$ \\int K(\\mathbf{x},\\mathbf{y}) g(\\mathbf{x}) g(\\mathbf{y}) d \\mathbf{x} d \\mathbf{y} \\ge 0 $$\n",
    "or equivalently for all $\\left\\{ \\mathbf{x_1},\\mathbf{x_2}, ... , \\mathbf{x_n} \\right\\}$,\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "K(\\mathbf{x_1},\\mathbf{x_2}) & \\cdots & K(\\mathbf{x_1},\\mathbf{x_n}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "K(\\mathbf{x_n},\\mathbf{x_1}) & \\cdots & K(\\mathbf{x_n},\\mathbf{x_n})\n",
    "\\end{bmatrix} \\ge 0 \\quad Positive \\: Semi \\: Definite $$ \n",
    "\n",
    "수학적으로 Kernel이 되는지 안되는지는 증명을 살펴봐야겠지만, 필요하다면 따로 확인을 하고 이미 알려지고 많이 사용되는 Kernel에 대해 살펴보자.\n",
    "\n",
    "## Some Kernels\n",
    "잘 알려진 그리고 많이 사용되는 커널 함수들은 다음과 같다.\n",
    "\n",
    "- 1.Linear Kernel : $K(\\mathbf{x_i},\\mathbf{x_j}) = \\mathbf{x_i} \\cdot \\mathbf{x_j}$\n",
    "\n",
    "- 2.Polynomial Kernel : $K(\\mathbf{x_i},\\mathbf{x_j}) = (\\mathbf{x_i} \\cdot \\mathbf{x_j}+c)^d$, $c>0$\n",
    "\n",
    "- 3.Sigmoid Kernel :  $K(\\mathbf{x_i},\\mathbf{x_j}) = tanh \\left\\{ a \\mathbf{x_i}^T \\mathbf{x_j}+ b \\right\\}$ , $a,b \\ge 0$\n",
    "\n",
    "- 4.Radial Basis Function Kernel : $K(\\mathbf{x_i},\\mathbf{x_j}) = exp \\left\\{ - \\frac{||\\mathbf{x_i}-\\mathbf{x_j}||^2}{2 \\sigma^2} \\right\\}$, $\\sigma \\ne 0$\n",
    "\n",
    "단, 각각에 사용된 $a,b,\\sigma$는 초매개변수(hyper-parameter)로서, 적절한 값을 지정해주어야 한다. 보통 이전의 Decision Tree에서 Overfitting을 방지하기 위해 Pruning을 했는데, 어떤 가지를 칠지를 정할 때 검증데이터(Validation Data)를 이용해서 정했다. 마찬가지로 초매개변수도 검증데이터를 이용해서 결정한다.\n",
    "\n",
    "적절한 Kernel을 선택하였다면, 이제 원래 데이터의 공간에서가 아닌 Feature Space에서의 최적화 문제로 바뀌게 되는데 Feature Mapping 대신에 Kernel을 이용하면 문제가 다음과 같이 표현된다.\n",
    "\n",
    "## Kernelized SVM\n",
    "\n",
    "\\begin{aligned}\n",
    "&max \\quad \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j (\\boldsymbol{\\Phi}(x_i) \\cdot \\boldsymbol{\\Phi}(x_j))\n",
    "\\\\ \\rightarrow &max \\quad \\sum_{i=1}^{N} \\lambda_i - \\frac{1}{2} \\sum_{i,j}^{N} \\lambda_i \\lambda_j y_i y_j K(\\mathbf{x_i},\\mathbf{x_j})\n",
    "\\\\ & subject \\: to \\quad \\sum_{i=1}^{N} \\lambda_i y_i=0 ~{and}~ \\lambda_i \\ge 0 \n",
    "\\end{aligned}\n",
    "\n",
    "원래 데이터 공간에서와 마찬가지로 Feature space에서의 최적의 해는 \n",
    "\n",
    "$$ \\mathbf{w}^* = \\sum_{i=1}^{N} {\\lambda_i}^* y_i \\boldsymbol{\\Phi}(\\mathbf{x_i})$$\n",
    "\n",
    "가 될테고, support vector를 이용해서 $b$를 계산을 완료했다고 하자.\n",
    "\n",
    "그렇다면 새로운 데이터 $\\mathbf{x_{new}}$에 대해 Kernelized SVM은 다음을 계산하게 된다.\n",
    "\n",
    "\\begin{aligned}\n",
    "y &= sign \\left\\{ (\\sum_{i=1}^{N} {\\lambda_i}^* y_i \\boldsymbol{\\Phi}(\\mathbf{x_i})) \\cdot \\boldsymbol{\\Phi}(\\mathbf{x_{new}}) + b \\right\\}\n",
    "\\\\ & = sign \\left\\{ \\sum_{i=1}^{N} {\\lambda_i}^* y_i \\boldsymbol{\\Phi}(\\mathbf{x_i}) \\cdot \\boldsymbol{\\Phi}(\\mathbf{x_{new}}) + b \\right\\}\n",
    "\\\\ & = sign \\left\\{ \\sum_{i=1}^{N} {\\lambda_i}^* y_i K(\\mathbf{x_i},\\mathbf{x_{new}}) + b \\right\\}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Reference\n",
    " - 알고리즘 중심의 머신러닝 가이드 제2판,스티븐 마슬랜드 지음, 강전형 옮김, 제이펍\n",
    " - J.H. Lee, Lectur Notes, http://bigs.skku.edu/bigs/menu4/reference.jsp?mode=view&article_no=303114&board_wrapper=%2Fbigs%2Fmenu4%2Freference.jsp&pager.offset=10&board_no=1423\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ \\int _ {P} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
