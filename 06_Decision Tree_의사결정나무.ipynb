{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree_의사결정나무"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall)\n",
    "스무고개 게임을 통해서 타이타닉 호 탑승자의 생존확률을 예측해보자.\n",
    "\n",
    "- Question.1 탑승자는 남자인가? 여자인가?\n",
    "- Question.2 나이가 9.5보다 많은가 적은가\n",
    "- Question.3 자녀의 수가 2.5명보다 많은가 적은가?\n",
    "- ....\n",
    "\n",
    "이렇게 물음에 물음을 더하다보면, 실제 타이타닉호에 탑승한 사람들을 분류할 수 있을 것이다.\n",
    "\n",
    "이러한 물음의 순서를 그림으로 나타내면 다음과 같다.\n",
    "\n",
    "Figure.1\n",
    "![](./image/06/06_01.png)\n",
    "이미지 출저:위키피디아\n",
    "\n",
    "그림의 모양이 위에서 부터 아래로 가면서 가지가 늘어나는 모양이 나무의 모양을 닮았다고 하여 이름을 의사결정나무(Decision Tree)라고 한다.\n",
    "\n",
    "Decision Tree는 if-then 문으로 표현이 가능하기 때문에 이해가 쉽고 직관적이다.\n",
    "\n",
    "위의 타이타닉 예제를 if then으로 나타내면 다음과 같다.\n",
    "\n",
    "if 남자가 아니라면 then 생존\n",
    "\n",
    "if 남자이고 나이가 9.5보다 크다면 then 사망\n",
    "\n",
    "if 남자이고 나이가 9.5보다 작고 자녀수가 2.5보다 크다면 then 사망\n",
    "\n",
    "....\n",
    "\n",
    "그렇다면, Decision Tree의 학습과정은 어떻게 일어나는가? 즉, 우리가 갖고있는 sample data(혹은 training data)로 부터 Tree를 어떻게 만들것인가를 살펴보아야 한다. 이를 Decision Rule(의사결정규칙)을 정한다고 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark)\n",
    "\n",
    "Tree를 만드는 방법에 따라 ID3, C4.5, C5.0, CHAID, CART등이 있지만, 차이는 무엇을 기준으로 가지를 치느냐이다.\n",
    "\n",
    "예를 들어 스무고개를 할 때 질문을 함으로써 정답에 대한 정보를 얼마나 많이 얻을 수 있느냐 없느냐를 고려하여야 한다.\n",
    "\n",
    "이런 정보를 얼마나 많이 얻느냐에 대한 것을 수치화하는 분야를 Information Theory에서 많이 다룬다.\n",
    "\n",
    "Data 집합의 Impurity(불순도)를 정의하는 방법이 여러가지가 있는데, 크게 Entropy와 Gini Index를 사용한다.\n",
    "\n",
    "결국 Decision Tree는 학습데이터의 Impurity를 줄이는 방향으로 Tree를 구성한다고 생각하면 된다.\n",
    "\n",
    "Decision Tree는 분류와 회귀 모두 사용이 가능하지만, 주로 분류문제에서 사용되므로 분류를 기준으로 설명하겠다.\n",
    "\n",
    "먼저 Entropy에 대해 살펴보자.\n",
    "\n",
    "주어진 Training data의 클래스의 비율을 $p_i$라고 할 때, 전체 Training Data의 Entropy H는 다음과 같다.\n",
    " \n",
    "$$ H = - \\sum_{i} p_i \\log_{2}{p_i} $$\n",
    "\n",
    "쉽게 말해 H가 높을 수록 현재의 Data 집합의 불순도가 높다는 뜻이고 이는 곧 클래스들을 분류하기가 쉽지 않다는 뜻이 된다.\n",
    "\n",
    "다음으로 Gini Index에 대해 살펴보자.\n",
    "\n",
    "마찬가지로 주어진 Training data의 클래스의 비율을 $p_i$라 할 때, 전체 Training Data의 Gini Index G는 다음과 같다.\n",
    "\n",
    "$$ G = 1 - \\sum_{i} {p_i}^2 $$\n",
    "\n",
    "Figure 2. 이진 분류에서 +클래스의 비율에 따른 엔트로피의 추이\n",
    "![](./image/06/06_02.png)\n",
    "이미지 출처:https://github.com/dsindex/blog/wiki/%5Bstatistics%5D--Entropy,-Relative-Entropy-and-Mutual-Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "\n",
    "현재 데이터의 불순도에 대한 수치를 정의 하였으니 이 불순도를 기준으로 하여 Tree를 구성해보고자 한다.\n",
    "\n",
    "학습데이터의 어떤 Feature를 기준으로 Tree를 구성할지를 불순도를 기준으로 한다고 했는데 \n",
    "\n",
    "학습 과정은 간단하다.\n",
    "\n",
    "어떤 Feature를 기준으로 데이터를 분류한 후 불순도가 얼마나 줄어드는지 확인 한 다음 가장 불순도가 많이 감소한 Feature를 선택하여 Tree를 구성한다.\n",
    "\n",
    "이 때, 불순도가 얼마나 감소하느냐를 다른말로 정보획득(Information Gain)이라고 하는데, 정의는 다음과 같다.\n",
    "\n",
    "전체 Training Data 집합을 $S$, Feature 들을 $(a_1, a_2, ... , a_n)$ 이라고 하자. 그리고 각 Feature $a_j$가 가질수 있는 값을 $\\left\\{ v_{j1},v_{j2}, ... , v_{jk(j)} \\right\\}$라 하자.\n",
    "\n",
    "이 때, $a_j = v_{ji}$에 대한 정보 획득량은 다음과 같다.\n",
    "\n",
    "$$Gain(S, a_j = v_{ji}) = Entropy(S) - \\frac{|S_{v_{ji}}|}{|S|} Entropy(S_{v_{ji}})-\\frac{|S_{v_{ji}^c}|}{|S|} Entropy(S_{v_{ji}^c}): \\quad {Entropy}\n",
    "\\\\Gain(S, a_j) = G(S) - \\frac{|S_{v_{ji}}|}{|S|} G(S_{v_{ji}})-\\frac{|S_{v_{ji}^c}|}{|S|} G(S_{v_{ji}^c}): \\quad {Gini \\, index}$$\n",
    "\n",
    "이 때 $S_{v_{ji}}$의 의미는 j 번째 Feature $a_j = v_{ji}$인 data의 집합을 의미하고 $S_{v_{ji}^c}$의 의미는 그 이외의 집합을 의미한다.\n",
    "\n",
    "복잡한 수식말고 예제를 통해 계산을 확인해보자.\n",
    "\n",
    "### Example\n",
    "![](./image/06/06_03.png)\n",
    "\n",
    "- 엔트로피\n",
    "$Entropy(S) = - \\frac{9}{14} \\log_{2}{\\frac{9}{14}} - \\frac{5}{14} \\log_{2}{(\\frac{5}{14})} = 0.940 $\n",
    "\n",
    "$Gain(S, Outlook = Sunny) = Entropy(S) - \\frac{5}{14} Entropy(S_{Sunny}) - \\frac{9}{14} Entropy(S_{{Sunny}^{c}})\n",
    "\\\\=0.94 - \\frac{5}{14}( - \\frac{2}{5} \\log_{2}{(\\frac{2}{5})} - \\frac{3}{5} \\log_{2}{(\\frac{3}{5})}) - \\frac{9}{14}( - \\frac{6}{9} \\log_{2}{(\\frac{6}{9})} - \\frac{3}{9} \\log_{2}{(\\frac{3}{9})}) \\approx 0.0028$\n",
    "\n",
    "- 지니 계수\n",
    "$G(S) = 1 - \\left( \\frac{9}{14} \\right)^2 - \\left( \\frac{5}{14} \\right)^2 \\approx 0.46 $\n",
    "\n",
    "$Gain(S, Outlook = Sunny) = G(S) - \\frac{5}{14} G(S_{Sunny}) - \\frac{9}{14} G(S_{{Sunny}^{c}})\n",
    "\\\\= 0.46 - \\frac{5}{14} ( 1 - \\left( \\frac{2}{5} \\right)^2 - \\left( \\frac{3}{5} \\right)^2) - \\frac{9}{14}(1 - \\left( \\frac{6}{9} \\right)^2 - \\left( \\frac{3}{9} \\right)^2) \\approx 0.0028$\n",
    "\n",
    "이런 방법으로 모든 Feature와 Feature가 가질 수 있는 모든 값들에 대해 정보 획득을 계산 한 다음 가장 큰 정보 획득량을 나타내는 Feature의 값을 기준으로 처음 Tree의 가지를 구성한다.\n",
    "\n",
    "이 때, Entropy를 사용하여 Information Gain을 계산하는 알고리즘을 ID3알고리즘이라고 하고,\n",
    "\n",
    "Gini Index를 사용하여 Information Gain을 계산하는 알고리즘을 CART라고 한다.\n",
    "\n",
    "Figure 3. sklearn의 Decision Tree를 이용해 PlayTennis 예제에 대한 Tree\n",
    "![](./image/06/06_04.png)\n",
    "이미지 출저: 김태우(vol12@skku.edu/010-4789-****/성균관대학교 선형모델연구실)\n",
    "\n",
    "## Remark)\n",
    "앞서 ID3와 CART 2가지 알고리즘에 대해 설명하였는데, Decision Tree를 형성하는 알고리즘은 이외에도 여러가지가 있다. \n",
    "\n",
    "참고로 다음과 같다.\n",
    "\n",
    "Figure 4. Decision Tree의 여러 알고리즘과 각각의 특징\n",
    "![](./image/06/06_05.png)\n",
    "이미지 출저:http://www.birc.co.kr/2017/01/11/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4decision-tree/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 장점\n",
    "-  Decision Tree는 분석 과정이 직관적이고 이해하기 쉽다. 대부분의 기계학습은 분석 결과에 대한 설명을 이해하기 어려운 블랙박스 모델인 반면,Decision Tree는 분석과정을 실제로 눈으로 관측할 수 있는 대표적인 화이트박스 모델이다.\n",
    "- 비교적 다른 알고리즘에 비해 계산비용이 낮아 데규모 데이터에서도 비교적 빠른 연산이 가능하다.\n",
    "- 수치형 자료와 범주형 자료 모두에 적용이 가능한 모델이다.\n",
    "\n",
    "## 단점\n",
    "- 최적의 Tree를 학습시키는 문제는 NPC 문제로 알려져 있다.\n",
    "<br> $\\Longrightarrow$ Greedy 알고리즘 사용: 최적해를 구하는 근사적 방법, 매 순간에 최적이라고 생각되는 것을 선택해 나가는 방식으로 진행\n",
    "- 많은 변수의 데이터를 일반화하지 않고 사용할 경우 복잡한 결정트리가 만들어질 수 있다.\n",
    "<br> $\\Longrightarrow$ Pruning(가지치기)를 사용하여 Overfitting을 방지 할 수 있다.\n",
    "- 데이터의 특성이 특정 변수에 수직/수평적으로 구분되지 못할 때 성능이 떨어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning_최하나님"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cinema4dr12.tistory.com/933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- https://ratsgo.github.io/machine%20learning/2017/03/26/tree/\n",
    "- http://www.kostat.go.kr/attach/journal/4-1-3.PDF\n",
    "- http://gentlej90.tistory.com/91\n",
    "- http://www.dodomira.com/2016/05/29/564/\n",
    "- http://www.birc.co.kr/2017/01/11/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4decision-tree/\n",
    "- 알고리즘 중심의 머신러닝 가이드 제2판_스티븐 마슬랜드 지음, 강전형 옮김_제이펍"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
