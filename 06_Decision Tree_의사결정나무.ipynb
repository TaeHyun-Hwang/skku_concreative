{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree_의사결정나무"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall)\n",
    "스무고개 게임을 통해서 타이타닉 호 탑승자의 생존확률을 예측해보자.\n",
    "\n",
    "- Question.1 탑승자는 남자인가? 여자인가?\n",
    "- Question.2 나이가 9.5보다 많은가 적은가\n",
    "- Question.3 자녀의 수가 2.5명보다 많은가 적은가?\n",
    "- ....\n",
    "\n",
    "이렇게 물음에 물음을 더하다보면, 실제 타이타닉호에 탑승한 사람들을 분류할 수 있을 것이다.\n",
    "\n",
    "이러한 물음의 순서를 그림으로 나타내면 다음과 같다.\n",
    "\n",
    "Figure.1\n",
    "![](./image/06/06_01.png)\n",
    "이미지 출저:위키피디아\n",
    "\n",
    "그림의 모양이 위에서 부터 아래로 가면서 가지가 늘어나는 모양이 나무의 모양을 닮았다고 하여 이름을 의사결정나무(Decision Tree)라고 한다.\n",
    "\n",
    "Decision Tree는 if-then 문으로 표현이 가능하기 때문에 이해가 쉽고 직관적이다.\n",
    "\n",
    "위의 타이타닉 예제를 if then으로 나타내면 다음과 같다.\n",
    "\n",
    "if 남자가 아니라면 then 생존\n",
    "\n",
    "if 남자이고 나이가 9.5보다 크다면 then 사망\n",
    "\n",
    "if 남자이고 나이가 9.5보다 작고 자녀수가 2.5보다 크다면 then 사망\n",
    "\n",
    "....\n",
    "\n",
    "그렇다면, Decision Tree의 학습과정은 어떻게 일어나는가? 즉, 우리가 갖고있는 sample data(혹은 training data)로 부터 Tree를 어떻게 만들것인가를 살펴보아야 한다. 이를 Decision Rule(의사결정규칙)을 정한다고 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark)\n",
    "\n",
    "Tree를 만드는 방법에 따라 ID3, C4.5, C5.0, CHAID, CART등이 있지만, 차이는 무엇을 기준으로 가지를 치느냐이다.\n",
    "\n",
    "예를 들어 스무고개를 할 때 질문을 함으로써 정답에 대한 정보를 얼마나 많이 얻을 수 있느냐 없느냐를 고려하여야 한다.\n",
    "\n",
    "이런 정보를 얼마나 많이 얻느냐에 대한 것을 수치화하는 분야를 Information Theory에서 많이 다룬다.\n",
    "\n",
    "Data 집합의 Impurity(불순도)를 정의하는 방법이 여러가지가 있는데, 크게 Entropy와 Gini Index를 사용한다.\n",
    "\n",
    "결국 Decision Tree는 학습데이터의 Impurity를 줄이는 방향으로 Tree를 구성한다고 생각하면 된다.\n",
    "\n",
    "Decision Tree는 분류와 회귀 모두 사용이 가능하지만, 주로 분류문제에서 사용되므로 분류를 기준으로 설명하겠다.\n",
    "\n",
    "먼저 Entropy에 대해 살펴보자.\n",
    "\n",
    "주어진 Training data의 클래스의 비율을 $p_i$라고 할 때, 전체 Training Data의 Entropy H는 다음과 같다.\n",
    " \n",
    "$$ H = - \\sum_{i} p_i \\log_{2}{p_i} $$\n",
    "\n",
    "쉽게 말해 H가 높을 수록 현재의 Data 집합의 불순도가 높다는 뜻이고 이는 곧 클래스들을 분류하기가 쉽지 않다는 뜻이 된다.\n",
    "\n",
    "다음으로 Gini Index에 대해 살펴보자.\n",
    "\n",
    "마찬가지로 주어진 Training data의 클래스의 비율을 $p_i$라 할 때, 전체 Training Data의 Gini Index G는 다음과 같다.\n",
    "\n",
    "$$ G = 1 - \\sum_{i} {p_i}^2 $$\n",
    "\n",
    "Figure 2. 이진 분류에서 +클래스의 비율에 따른 엔트로피의 추이\n",
    "![](./image/06/06_02.png)\n",
    "이미지 출처:https://github.com/dsindex/blog/wiki/%5Bstatistics%5D--Entropy,-Relative-Entropy-and-Mutual-Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART\n",
    "\n",
    "현재 데이터의 불순도에 대한 수치를 정의 하였으니 이 불순도를 기준으로 하여 Tree를 구성해보고자 한다.\n",
    "\n",
    "학습데이터의 어떤 Feature를 기준으로 Tree를 구성할지를 불순도를 기준으로 한다고 했는데 \n",
    "\n",
    "학습 과정은 간단하다.\n",
    "\n",
    "어떤 Feature를 기준으로 데이터를 분류한 후 불순도가 얼마나 줄어드는지 확인 한 다음 가장 불순도가 많이 감소한 Feature를 선택하여 Tree를 구성한다.\n",
    "\n",
    "이 때, 불순도가 얼마나 감소하느냐를 다른말로 정보획득(Information Gain)이라고 하는데, 정의는 다음과 같다.\n",
    "\n",
    "전체 Training Data 집합을 $S$, Feature 들을 $(a_1, a_2, ... , a_n)$ 이라고 하자. 그리고 각 Feature $a_j$가 가질수 있는 값을 $\\left\\{ v_{j1},v_{j2}, ... , v_{jk(j)} \\right\\}$라 하자.\n",
    "\n",
    "이 때, $a_j = v_{ji}$에 대한 정보 획득량은 다음과 같다.\n",
    "\n",
    "$$Gain(S, a_j = v_{ji}) = Entropy(S) - \\frac{|S_{v_{ji}}|}{|S|} Entropy(S_{v_{ji}})-\\frac{|S_{v_{ji}^c}|}{|S|} Entropy(S_{v_{ji}^c}): \\quad {Entropy}\n",
    "\\\\Gain(S, a_j) = G(S) - \\frac{|S_{v_{ji}}|}{|S|} G(S_{v_{ji}})-\\frac{|S_{v_{ji}^c}|}{|S|} G(S_{v_{ji}^c}): \\quad {Gini \\, index}$$\n",
    "\n",
    "이 때 $S_{v_{ji}}$의 의미는 j 번째 Feature $a_j = v_{ji}$인 data의 집합을 의미하고 $S_{v_{ji}^c}$의 의미는 그 이외의 집합을 의미한다.\n",
    "\n",
    "복잡한 수식말고 예제를 통해 계산을 확인해보자.\n",
    "\n",
    "### Example)\n",
    "![](./image/06/06_03.png)\n",
    "\n",
    "- 엔트로피\n",
    "$Entropy(S) = - \\frac{9}{14} \\log_{2}{\\frac{9}{14}} - \\frac{5}{14} \\log_{2}{(\\frac{5}{14})} = 0.940 $\n",
    "\n",
    "$Gain(S, Outlook = Sunny) = Entropy(S) - \\frac{5}{14} Entropy(S_{Sunny}) - \\frac{9}{14} Entropy(S_{{Sunny}^{c}})\n",
    "\\\\=0.94 - \\frac{5}{14}( - \\frac{2}{5} \\log_{2}{(\\frac{2}{5})} - \\frac{3}{5} \\log_{2}{(\\frac{3}{5})}) - \\frac{9}{14}( - \\frac{6}{9} \\log_{2}{(\\frac{6}{9})} - \\frac{3}{9} \\log_{2}{(\\frac{3}{9})}) \\approx 0.0028$\n",
    "\n",
    "- 지니 계수\n",
    "$G(S) = 1 - \\left( \\frac{9}{14} \\right)^2 - \\left( \\frac{5}{14} \\right)^2 \\approx 0.46 $\n",
    "\n",
    "$Gain(S, Outlook = Sunny) = G(S) - \\frac{5}{14} G(S_{Sunny}) - \\frac{9}{14} G(S_{{Sunny}^{c}})\n",
    "\\\\= 0.46 - \\frac{5}{14} ( 1 - \\left( \\frac{2}{5} \\right)^2 - \\left( \\frac{3}{5} \\right)^2) - \\frac{9}{14}(1 - \\left( \\frac{6}{9} \\right)^2 - \\left( \\frac{3}{9} \\right)^2) \\approx 0.0028$\n",
    "\n",
    "이런 방법으로 모든 Feature와 Feature가 가질 수 있는 모든 값들에 대해 정보 획득을 계산 한 다음 가장 큰 정보 획득량을 나타내는 Feature의 값을 기준으로 처음 Tree의 가지를 구성한다.\n",
    "\n",
    "이 때, Entropy를 사용하여 Information Gain을 계산하는 알고리즘을 ID3알고리즘이라고 하고,\n",
    "\n",
    "Gini Index를 사용하여 Information Gain을 계산하는 알고리즘을 CART라고 한다.\n",
    "\n",
    "Figure 3. sklearn의 Decision Tree를 이용해 PlayTennis 예제에 대한 Tree\n",
    "![](./image/06/06_04.png)\n",
    "이미지 출저: 최황김\n",
    "\n",
    "# Remark)\n",
    "앞서 ID3와 CART 2가지 알고리즘에 대해 설명하였는데, Decision Tree를 형성하는 알고리즘은 이외에도 여러가지가 있다. \n",
    "\n",
    "참고로 다음과 같다.\n",
    "\n",
    "Figure 4. Decision Tree의 여러 알고리즘과 각각의 특징\n",
    "![](./image/06/06_05.png)\n",
    "이미지 출저:http://www.birc.co.kr/2017/01/11/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4decision-tree/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 장점\n",
    "-  Decision Tree는 분석 과정이 직관적이고 이해하기 쉽다. 대부분의 기계학습은 분석 결과에 대한 설명을 이해하기 어려운 블랙박스 모델인 반면,Decision Tree는 분석과정을 실제로 눈으로 관측할 수 있는 대표적인 화이트박스 모델이다.\n",
    "- 비교적 다른 알고리즘에 비해 계산비용이 낮아 데규모 데이터에서도 비교적 빠른 연산이 가능하다.\n",
    "- 수치형 자료와 범주형 자료 모두에 적용이 가능한 모델이다.\n",
    "\n",
    "## 단점\n",
    "- 최적의 Tree를 학습시키는 문제는 NPC 문제로 알려져 있다.\n",
    "<br> $\\Longrightarrow$ Greedy 알고리즘 사용: 최적해를 구하는 근사적 방법, 매 순간에 최적이라고 생각되는 것을 선택해 나가는 방식으로 진행\n",
    "- 많은 변수의 데이터를 일반화하지 않고 사용할 경우 복잡한 결정트리가 만들어질 수 있다.\n",
    "<br> $\\Longrightarrow$ Pruning(가지치기)를 사용하여 Overfitting을 방지 할 수 있다.\n",
    "- 데이터의 특성이 특정 변수에 수직/수평적으로 구분되지 못할 때 성능이 떨어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과대적합 문제_Over-fitting in Decision Tree\n",
    "\n",
    "일반적으로 Tree는 모든 클래스가 완전히 분리 될 때 까지(모든 Leaf 노드가 순수 노드가 될 때까지) 진행되므로 아래 그림과 같이 모델이 매우 복잡해지고 학습데이터에 과적합(Overfitting)되는 문제가 생긴다.\n",
    "\n",
    "\n",
    "- Figure5. Overfitting in Decision Tree\n",
    "![](./image/06/06_07.png)\n",
    "이미지출처: https://tensorflow.blog/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2-3-5-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC/\n",
    "\n",
    "## Pruning_가지치기\n",
    "\n",
    "Decision Tree에서 Overfitting 문제를 해결하는 방법은 두가지 방법이 있다.\n",
    "\n",
    "하나는 학습데이터를 완벽하게 분류하는 시점에 도달하기 전에 Tree를 멈추는 방법(Pre-Pruning)이고 다른 하나는 Tree를 완성한 이후에 가지치기를 하는 방법(Post Pruning)이다.\n",
    "\n",
    "### Pre-Pruning_사전 가지치기\n",
    "\n",
    "Pre-Pruning은 Tree의 최대 깊이나 가지의 최대 갯수를 제한하거나, 또는 노드가 분할하기 위한 포인트의 최소 갯수를 지정하는 방법을 말한다.\n",
    "\n",
    "scikit-learn에서 제공하는 Decision Tree는 Pre-pruning만 지원한다. 그러나 이 방법은 Tree의 성장을 멈춰야 하는 시점을 파악하기 어렵기 때문에 Post Pruning를 많이 사용한다.\n",
    "\n",
    "### Post-Pruning_사후 가지치기\n",
    "\n",
    "Post-Pruning에서는 Training set과 Validation set을 사용한다. 여러 종류가 있는데, 대표적인 방법은 다음과 같다.\n",
    "\n",
    "__Reduced Error Pruning__\n",
    "\n",
    "각 노드가 모두 pruning의 대상이 되며 해당 노드를 pruning 했을 때의 Tree의 성능이 pruning 하기 전의 Tree보다 성능이 나빠지지 않는 경우 노드의 pruning을 결정한다.(이때 validation set을 사용하여 성능을 측정한다.)\n",
    "\n",
    "- Figure 9. 가지(노드)의 수에 따른 Error\n",
    "![](./image/06/06_10.png)\n",
    "Figure 9 에서 노드 수를 줄일 수록 Test set의 accuracy가 증가함을 알 수 있다.\n",
    "\n",
    "\n",
    "## Validation Data_검증데이터\n",
    "\n",
    "지난시간에 과대적합(overfitting)문제에 대해 간단히 설명하였다. 과대적합은 문자 그대로 모델이 너무 과도하게 학습데이터에 학습되어서 새로운 데이터에 맞지 않는(즉, 일반화 되지 않는) 문제를 말한다. 이러한 과대적합문제를 해결하기 위한 방법 중 하나가 Validation을 이용하는 방법이다.\n",
    "\n",
    "- Figure 6. Overfitting \n",
    "![](./image/06/06_06.png)\n",
    "이미지출처: http://sanghyukchun.github.io/59/\n",
    "\n",
    "일반적으로 모든 학습데이터로 모델을 학습시키면 시킬 수록 학습데이터에 대한 Error는 줄어든다.\n",
    "\n",
    "- Figure 7. Training Error\n",
    "![](./image/06/06_08.png)\n",
    "이미지출처:http://rfriend.tistory.com/188\n",
    "\n",
    "그러나 이 경우 모델이 Underfitting인지(학습이 잘 되었는지), Overfitting인지(학습이 과하게 되었는지) 가늠하기 힘들다.\n",
    "\n",
    "이 때, 이러한 Overfitting을 탐지하기 위해 사용하는 Data를 Validation Data(검증 데이터)라고 한다.\n",
    "\n",
    "## How to prevent Overfitting\n",
    "\n",
    "주어진 Data set을 Training set, Validation set, Test set 3개의 set으로 구분을 한 후\n",
    "\n",
    "(보통 Train:Validataion:Test의 비율은 5:2.5:2.5 혹은 5:2:2 정도이나 Data 자체가 적을 땐 Training을 더 늘리기도 한다)\n",
    "\n",
    " 1. Training set을 이용하여 모델을 학습시키고\n",
    " 2. Validation set을 가지고 1번에서 학습된 모델의 에러를 조사하고, 과적합의 여부를 판단한다.(Validataion의 Error가 감소하다 증가하는 부분에서 Overfitting이 생겼다고 본다)\n",
    " 3. Test set을 이용하여 1번과 2번을 이용하여 만든 최종 모델의 성능을 파악한다. -> 최종 모델 성능 평가용\n",
    " \n",
    "- Figure. 8\n",
    "    ![](./image/06/06_09.png)\n",
    "이미지출처:http://rfriend.tistory.com/188\n",
    "\n",
    "위 그림은 Training set으로 만든 모델에 Validation set 데이터를 적용해 예측한 Error를 측정한 그래프이다. Trainin set의 Error는 학습이 계속될수록 감소하지만 Validation set의 Error은 줄어들다가 어느 순간 부터는 증가하게 된다. 바로 이 변곡점이 과대적합이 시작되는 지점으로 생각할 수 있다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example_DIY)\n",
    "다음과 같이 Income과 Lot size에 따른 제초기 기계의 보유여부를 나타낸 표이다.\n",
    "\n",
    "![](./image/06/06_11.png)\n",
    "\n",
    "Gini Index를 기준으로 직접 계산하여 Tree를 구성한 후 Python을 이용하여 Tree를 구성한 후 비교해보세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- https://ratsgo.github.io/machine%20learning/2017/03/26/tree/\n",
    "- http://www.kostat.go.kr/attach/journal/4-1-3.PDF\n",
    "- http://gentlej90.tistory.com/91\n",
    "- http://www.dodomira.com/2016/05/29/564/\n",
    "- http://www.birc.co.kr/2017/01/11/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4decision-tree/\n",
    "- 알고리즘 중심의 머신러닝 가이드 제2판_스티븐 마슬랜드 지음, 강전형 옮김_제이펍\n",
    "- https://m.blog.naver.com/PostView.nhn?blogId=cake54&logNo=40108485351&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F\n",
    "- Machine Learning,first edition,Tom M.Mitchell 지음,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
