{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression_선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀는 주어진 데이터 집합 {$((x_{i1},\\dots,x_{ij}), y_i), \\quad i=1,2,...,n$}에 대해, 종속변수 $y_i$와 독립변수 $x_{ik}$들 사이에 __선형관계__를 모델링하는 것을 말한다.\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + ... +\\beta_k x_{ik} = x_i^T\\beta$$\n",
    "여기서 __선형관계__란 단순히 $y_i=x_i^T\\beta$ 그래프가 직선이고 $y_i$가 $x_i$의 선형함수일 것이라고 생각하는 것은 잘못이다. 예를 들어 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2 $ 역시 $y$가 $x$와 $x^2$에 관한 선형이기 때문에 선형회귀라고 한다.\n",
    "\n",
    "### Note that) \n",
    "여기서 선형관계란 선형결합을 뜻한다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "독립변수의 개수 k에 따라 k=1이면 단순 선형 회귀(Simple linear regression), k>1이면 다중 선형 회귀(Multiple inear regression)으로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A simple linear regression model\n",
    "\n",
    "### Model\n",
    "\n",
    "우리에게 n개의 샘플 ($(x_i, y_i), \\quad i=1,2,...,n$)이 주어져있다고 하자.\n",
    "\n",
    "이 때 우리는 $y_i$를 추정하는 데 $\\hat{y_i} = \\beta_0+\\beta_1x_i$를 이용하고자 한다. \n",
    "\n",
    "-Figure 1 선형회귀분석\n",
    "![](./image/04/04_01.png)\n",
    "이미지 출저:https://blog.naver.com/mykepzzang/220935001644\n",
    "\n",
    "### Performance measure(cost function 또는 loss function)\n",
    "\n",
    "이 때 현재의 $\\beta_0, \\beta_1$이 얼마나 적당한지를 측정하는 Performance measure로써 우리는 MSE(Mean Squared Error, 평균제곱오차)를 사용한다.\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 $$\n",
    "\n",
    "이제 우리가 찾고자 하는 $\\beta_0, \\beta_1$은 MSE를 최소로 하는 값을 선택하면 된다.\n",
    "\n",
    "### Training\n",
    "\n",
    "1. __Normal Equation 이용하는 방법__\n",
    "\n",
    "    MSE는 $\\beta_0, \\beta_1$에 대해서 convex function 이므로 극값에서 최소값을 갖는다.\n",
    "\n",
    "    From $$\\frac{\\partial{MSE}}{\\partial{\\beta_0}} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i) =0 \\quad \\rightarrow \\quad \\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$$\n",
    "\n",
    "    From $$\\frac{\\partial{MSE}}{\\partial{\\beta_1}} = \\frac{-2}{n} \\sum_{i=1}^{n} x_i (y_i - \\beta_0 - \\beta_1 x_i) =0 \\quad \\rightarrow \\quad \\beta_1 = \\frac{\\sum_{i=1}^{n} x_i y_i - n \\bar{x}\\bar{y}}{\\sum_{i=1}^{n} {x_i}^2 - n {\\bar{x}}^2}$$\n",
    "\n",
    "    where $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$, $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$\n",
    "\n",
    "    식을 조금 간단히 하기 위해 새로운 Notation을 도입해보자.\n",
    "\n",
    "    #### Notation)\n",
    "    - $S_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $ : 편차 제곱의 합\n",
    "    - $S_{yy} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $ \n",
    "    - $S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{n}( x_i y_i - n \\bar{x} \\bar{y}) $\n",
    "\n",
    "    이 기호를 사용하여 $\\beta_0, \\beta_1$을 나타내면 다음과 같다.\n",
    "\n",
    "    $$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\\\ \\beta_1 = \\frac{S_{xy}}{S_{xx}}$$\n",
    "    \n",
    "2. __Iterative methods__\n",
    "    \n",
    "    이렇게 Explicit 한 Form 말고도, 반복적인 방법으로도 답을 구할 수 있다.\n",
    "\n",
    "    초기 $\\beta_0, \\beta_1$을 무작위로 정한다음 MSE를 최적화 방법(예를 들면 Gradient Descent 방법: 후에 다시 다뤄보도록 하자)을 이용하여 구할 수 도 있다.\n",
    "    - Figure2. 선형회귀의 반복적인 업데이트\n",
    "![](./image/04/04_02.png)\n",
    "이미지 출저:https://medium.com/mathpresso/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-4-%ED%9A%8C%EA%B7%80-%EB%B6%84%EC%84%9D-regression-1-6d6cc0aaa483\n",
    "\n",
    "    - Figure3. Normal Equationr과 Iterative methods 비교\n",
    "![](./image/04/04_03.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계적 접근\n",
    "\n",
    "지금까지 주어진 데이터의 관계를 가장 적절하게 나타내는 선형식을 MSE의 관점에서 찾아보았다.\n",
    "\n",
    "이제 확률의 관점에서 한번 생각해보자.\n",
    "\n",
    "종속변수 $y_i$에 대해 추정 값 $\\hat{y}_{i} = \\beta_0 + \\beta_1 x_i$과의 차이(에러)를 $e_i$라 하자.\n",
    "\n",
    "$$ y_i= \\hat{y}_{i}+e_i$$\n",
    "\n",
    "이 때 이 $e_i$가 평균이 0이고 분산이 $\\sigma^2$인 정규분포를 따른다고 가정하자.\n",
    "\n",
    "$$e_i \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "그러면 어떤 $\\beta_0, \\beta_1$의 값이 주어진 데이터의 가능도(Likelihood)를 크게 하는지 살펴볼 필요.\n",
    "\n",
    "다음 식으로부터\n",
    "\n",
    "$$y_i = \\hat{y}_{i} + e_i = \\beta_0 + \\beta_1 x_i + e_i \\quad {and} \\quad e_i \\sim N(0, \\sigma^2)$$ \n",
    "\n",
    "다음을 얻을 수 있다\n",
    "\n",
    "$$ y_i \\sim N(\\beta_0 + \\beta_1 x_i , \\sigma^2 )$$\n",
    "\n",
    "이 때 주어진 데이터의 독립을 가정하면 가능도 함수는 다음과 같다.\n",
    "\n",
    "$${likelihood}= p(y_1, ... ,y_n | \\beta_0, \\beta_1, \\sigma^2) = \\prod_{i=1}^n p(y_i|\\beta_0, \\beta_1, \\sigma^2)$$\n",
    "\n",
    "$$= \\prod_{i=1}^n N(\\beta_0 + \\beta_1 x_i, \\sigma^2) $$\n",
    "\n",
    "$$= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{\\frac{-(y_i - (\\beta_0 + \\beta_1 x_i ))^2}{2 \\sigma^2}}$$\n",
    "\n",
    "$$= \\left(\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\right)^n e^{\\frac{-1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - (\\beta_0 +\\beta_1 x_i))^2}$$\n",
    "\n",
    "계산의 편의를 위해 양변에 로그를 취하면,\n",
    "\n",
    "$${log Likelihood} = -n log(\\sqrt{2 \\pi} \\sigma) - \\frac{1}{2 \\sigma^2} \\sum_{i} (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n",
    "\n",
    "이제 최적의 $\\beta_0, \\beta_1$은 위의 log likelihood값을 최대로 하는 값으로 택하면 된다. 그런데 이때 에러의 분산($\\sigma^2$)을 상수로 가정한다면, 로그 가능도를 최대화하는 것은 $\\frac{1}{2 \\sigma^2} \\sum_{i} (y_i - (\\beta_0 + \\beta_1 x_i))^2$ 이부분을 최소화 하는것 과 같다. 즉, 앞에서 한 MSE의 목적함수와 같게 된다.\n",
    "\n",
    "$$ {Maximizing\\: log\\: liklihood} \\Longleftrightarrow {minimizing \\: sum \\: of \\: square \\: error(MSE)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "이번에는 k개의 독립변수가\n",
    "있다고 가정하자.\n",
    "\n",
    "우리에게 다음과 같은 n개의 샘플이 주어졌다고 하자. 이 때, $x$는 독립변수 $y$는 종속변수이다.\n",
    "\n",
    "$((x_{i1},x_{i2}, ... , x_{ik}), y_i), \\quad i=1,2,...,n$\n",
    "\n",
    "이 때 우리는 $y_i$를 추정하는 데 $\\hat{y_i} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_k x_{ik}$을 이용하려고 한다.\n",
    "\n",
    "간단히 하기 위해 Notation을 \n",
    "\n",
    "- $\\mathbb{x}_{i} = [1,x_{i1},x_{i2}, ... ,x_{ik}]^T ,\\quad i = 1,2, ... , n$\n",
    "- $X = \\begin{bmatrix} \\mathbb{x_1}^T \\\\ \\vdots \\\\  \\mathbb{x_n}^T \\end{bmatrix} = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1k} \\\\ & & \\vdots \\\\ 1 & x_{n1} &  \\cdots & x_{nk} \\end{bmatrix}$\n",
    "- $\\mathbb{\\beta} = [\\beta_0, \\beta_1, ... , \\beta_k]^T$\n",
    "- $\\mathbb{\\hat{y}} = [\\hat{y_1}, ... ,\\hat{y_n}]^T$\n",
    "- $\\mathbb{y} = [y_1, ... ,y_n]^T$\n",
    "\n",
    "$ \\Longrightarrow \\quad \\mathbb{\\hat{y}} = X \\mathbb{\\beta}, \\quad i.e) \\ \\  \\mathbb{\\hat{y}}_{i} =  \\sum_{j=0}^{k} x_{ij}\\beta_j, \\quad i=1,2,...,n$\n",
    "\n",
    "이 때 MSE는 다음과 같다.\n",
    "\n",
    "$$ MSE =\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$ \n",
    "\n",
    "그러면 \n",
    "\\begin{aligned}\n",
    " \\frac{\\partial MSE}{\\partial \\beta_j} \n",
    " &= -\\frac{2}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})x_{ij}\\\\ \n",
    " &= -\\frac{2}{n}[x_{1j} \\quad\\cdots\\quad x_{nj}](\\mathbb{y}-\\mathbb{\\hat{y}})\\\\\n",
    " &= 0 \\  \\quad for \\ j = 0,1,2, ... , k\n",
    "\\end{aligned}\n",
    " \n",
    "모든 j에 대해 나타내면\n",
    "$$0=-\\frac{2}{n}\n",
    "\\left[\\begin{array}{cccc}\n",
    "1&1&\\cdots&1\\\\\n",
    "x_{11}&x_{12}&\\cdots&x_{1n}\\\\\n",
    "\\vdots&\\vdots&&\\vdots\\\\\n",
    "x_{1k}&x_{2k}&\\cdots&x_{nk}\n",
    "\\end{array}\\right](\\mathbb{y}-\\mathbb{\\hat{y}})=-\\frac{2}{n}X^T(\\mathbb{y}-\\mathbb{\\hat{y}})=-\\frac{2}{n}X^T(\\mathbb{y}-X\\beta)\n",
    "$$\n",
    "\n",
    "$\\beta$ 는 다음과 같이 결정 된다.\n",
    "\n",
    "$$ X^T X \\beta = X^T \\mathbb{y}\\quad \\Longrightarrow \\quad \\beta = (X^T X)^{-1} X^T \\mathbb{y} $$\n",
    "\n",
    "\n",
    "- Figure3. k=2일 때 다중회귀의 반복적인 업데이트\n",
    "![](./image/04/04_04.png)\n",
    "이미지 출저: https://medium.com/mathpresso/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-4-%ED%9A%8C%EA%B7%80-%EB%B6%84%EC%84%9D-regression-1-6d6cc0aaa483\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evalating Model\n",
    "\n",
    "회귀모형이 얼마나 정확한지 적합한지 판단하는 척도는 결정계수(R-Square: $R^2$)로 나타낸다.\n",
    "결정계수는  $\\mathbb{y}$의 총 변동량을 $\\mathbb{\\hat{y}}$가 얼마나 잘 설명하는지를 나타내고 수식은 아래와 같다.\n",
    "\n",
    "$$R^2=1-\\frac{\\sum(y_i-\\hat{y_i}^2)}{\\sum(y_i-\\bar{y}^2)} $$\n",
    "\n",
    "\n",
    "$R^2$ 값은 0과 1사이이며 1에 가까울 수록 모델이 데이터를 잘 설명한다고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting_과적합\n",
    "\n",
    "단순회귀분석에서 주어진 데이터$(x_i, y_i)$에 대해 1차 선형회귀를 생각해보자.\n",
    "\n",
    "Figure.5 1차 선형회귀\n",
    "![](./image/04/04_05.png)\n",
    "\n",
    "이 때 주어진 데이터를 2차항을 붙여 2차 선형회귀를 한다고 생각해보자.\n",
    "\n",
    "즉, $\\hat{y} = \\beta_0 + \\beta_1 x +\\beta_2 x^2$으로 추정한다고 생각해보자.\n",
    "\n",
    "그럴 경우 적절한 회귀식은 다음과 같게 나타날 것이다.\n",
    "\n",
    "Figure.6 2차 선형회귀\n",
    "![](./image/04/04_06.png)\n",
    "\n",
    "이럴 경우 1차 선형회귀 때보다 좀 더 데이터의 분포를 잘 설명(학습데이터에 대한 MSE값이 작게 나온다)한다고 생각할 수 있다.\n",
    "\n",
    "이 때, 학습 데이터에 대한 MSE를 더 낮추기 위해 조금 더 고차항의 항을 붙여 학습 데이터를 추정한다고 생각해보자. \n",
    "\n",
    "Figure.7 고차 선형회귀\n",
    "![](./image/04/04_07.png)\n",
    "Figure5,6,7 이미지 출저:http://sanghyukchun.github.io/59/\n",
    "\n",
    "이 경우에 우리는 학습시킨 회귀식이 학습데이터의 모양은 잘 설명하지만 이 모델이 좋은 모델이라고 이야기하기는 힘들 것이다.\n",
    "\n",
    "이렇게 학습시킨 모델이 학습데이터에 대해서 과하게 학습되는 현상을 Overfitting(과적합) 이라고 한다.\n",
    "\n",
    "이렇게 과적합된 모델은 새로운 데이터에 대해서는 좋은 성능을 보이지 않게 될 것이다. \n",
    "\n",
    "이러한 현상을 우리는 시험 데이터를 이용하여 확인할 수 있다.\n",
    "\n",
    "Figure.8 학습데이터와 시험데이터에 따른 에러와 과적합의 관계\n",
    "![](./image/04/04_08.png)\n",
    "이미지 출저:http://freesearch.pe.kr/wp-content/uploads/model_complexity_error_training_test.jpg\n",
    "\n",
    "위 그림에서 모델의 복잡도(complexity of model)이란 여기서는 선형회귀식의 차수라고 생각하면 된다.\n",
    "\n",
    "모델의 차수가 크면 클수록 학습데이터에 대한 에러는 점점 감소하는걸 알 수 있지만, 어느 시점이 지난 경우 시험 데이터에 대해서는 에러가 증가하는 경향이 보인다. 이 때 우리는 이 모델이 과적합이 되었다고 이야기를 한다.\n",
    "\n",
    "오버피팅을 다루는 방법만을 따로 모아도 하나의 학문 분야가 될 만큼 오버피팅은 머신러닝에서 중요한 문제다.\n",
    "\n",
    "때문에 오버피팅을 줄이는 많은 방법이 있는데 그 중에 하나인 정규화(Regularization)에 대해서 설명하고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization_규제\n",
    "\n",
    "정칙화란, 그림8에서 처럼 적절한 모델의 복잡도를 찾기 \n",
    "위해 목적함수(혹은 손실함수, 비용함수)에 적절한 penalty(제약?)을 붙이는 방법을 이야기 한다.\n",
    "\n",
    "Linear Regression에서의 Regularization은 아래와 같다\n",
    "\n",
    "#### $$Loss=MSE+\\lambda||w||$$\n",
    "\n",
    "이 때 추가되는 penalty에 따라 $L_2$-Regularization, $L_1$-Regularization 등의 이름이 붙는다.\n",
    "\n",
    "\n",
    "\n",
    "- __L2(Ridge)-regularization__\n",
    "\n",
    "    Loss funtion으로 MSE에 계수의 $L_2$-norm을 더해준 형태를 사용한다.\n",
    "\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{i=1}^{2}  w_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 + \\lambda \\lVert w \\rVert _2^2 ~~,~~ w = [\\beta_0, \\beta_1]^T$$\n",
    "\n",
    "\n",
    "    을 사용한다.\n",
    "\n",
    "- __L1(Lasso)-regularization__\n",
    "\n",
    "    L2(Ridge)-regularization과 비슷하게 Loss funtion을 MSE에 계수의 L1-norm term을 더해준 형태를 사용한다.\n",
    "\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{i=1}^{2} \\left\\vert w_i \\right\\vert = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 + \\lambda \\lVert w \\rVert_1 ~~,~~ w = [\\beta_0, \\beta_1]^T $$\n",
    "\n",
    "\n",
    "    을 사용한다.\n",
    "\n",
    "### Weight Decay_가중치 감소\n",
    "\n",
    "우선, 2차 함수를 하나 생각해보자.\n",
    "\n",
    "$$y = 10x^2 + 10x$$\n",
    "\n",
    "Figure.9 $y=10x^2 + 10x$ 그래프\n",
    "![](./image/04/04_09.png)\n",
    "\n",
    "이 때 같은 항을 사용하면서 항의 계수의 크기를 줄이면 그래프의 모양이 어떻게 되는지 살펴보자.\n",
    "\n",
    "Figure.10 $y=10x^2+10x$, $y=x^2+x$, $y=0.1x^2+0.1x$의 그래프\n",
    "![](./image/04/04_10.png)\n",
    "\n",
    "즉, 가중치 값이 낮을 수록 그래프가 완만해짐을 알 수 있다.\n",
    "\n",
    "그러므로 우리는 높은 차원의 그래프를 이용하되 가중치의 크기를 낮추면 완만하게 학습데이터에 적합한 모델을 만들 수 있을거란 생각을 할 수 있다.\n",
    "\n",
    "때문에 학습데이터에 대한 MSE에 가중치의 크기를 penalty로 붙이는 방법을 생각해보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- https://tensorflow.blog/2-%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0-%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-first-contact-with-tensorflow/\n",
    "- https://m.blog.naver.com/PostView.nhn?blogId=istech7&logNo=50152984368&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F\n",
    "- Lecture note of Machine Learning lectured by Prof. 길이만"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
